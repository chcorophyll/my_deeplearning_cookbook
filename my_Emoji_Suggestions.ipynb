{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my_Emoji Suggestions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chcorophyll/my_deeplearning_cookbook/blob/master/my_Emoji_Suggestions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYAY4jM9QljN",
        "colab_type": "code",
        "outputId": "c515ea3e-115f-4fbf-f7e4-96be3bd45ff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!git clone https://github.com/chcorophyll/deep_learning_cookbook.git"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'deep_learning_cookbook' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_LRx2fcRbUz",
        "colab_type": "code",
        "outputId": "e31c1fc3-9648-4d06-ac4c-57e62229df0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deep_learning_cookbook\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjKATigrRYMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "path_org = os.getcwd()\n",
        "data_path = os.path.join(path_org, \"deep_learning_cookbook\")\n",
        "os.chdir(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1PpnXdeR6tT",
        "colab_type": "text"
      },
      "source": [
        "**Trying out a simple learner**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64r4Q4pvR5mZ",
        "colab_type": "code",
        "outputId": "5a01f6f2-7648-4dc2-8eb9-dbd2fcbd9d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import pandas as pd\n",
        "from keras.utils.data_utils import get_file\n",
        "import nb_utils\n",
        "\n",
        "emotion_csv = get_file(\"tetx_emotion.csv\", \n",
        "                       \"https://www.crowdflower.com/wp-content/uploads/2016/07/text_emotion.csv\")\n",
        "emotion_df = pd.read_csv(emotion_csv)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.crowdflower.com/wp-content/uploads/2016/07/text_emotion.csv\n",
            "4399104/4394791 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j4cUy88SsI7",
        "colab_type": "code",
        "outputId": "d98672ee-5259-467b-ab71-4aca789efda7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "emotion_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>author</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1956967341</td>\n",
              "      <td>empty</td>\n",
              "      <td>xoshayzers</td>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1956967666</td>\n",
              "      <td>sadness</td>\n",
              "      <td>wannamama</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1956967696</td>\n",
              "      <td>sadness</td>\n",
              "      <td>coolfunky</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1956967789</td>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>czareaquino</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1956968416</td>\n",
              "      <td>neutral</td>\n",
              "      <td>xkilljoyx</td>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     tweet_id  ...                                            content\n",
              "0  1956967341  ...  @tiffanylue i know  i was listenin to bad habi...\n",
              "1  1956967666  ...  Layin n bed with a headache  ughhhh...waitin o...\n",
              "2  1956967696  ...                Funeral ceremony...gloomy friday...\n",
              "3  1956967789  ...               wants to hang out with friends SOON!\n",
              "4  1956968416  ...  @dannycastillo We want to trade with someone w...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouCFw47oVE6d",
        "colab_type": "code",
        "outputId": "98e8487b-8ff8-4b62-9b38-482b895b45ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "emotion_df.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40000 entries, 0 to 39999\n",
            "Data columns (total 4 columns):\n",
            "tweet_id     40000 non-null int64\n",
            "sentiment    40000 non-null object\n",
            "author       40000 non-null object\n",
            "content      40000 non-null object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 1.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9lo5AMsU4hP",
        "colab_type": "code",
        "outputId": "e5f003a7-5b6c-4153-c033-298ae27901a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "emotion_df[\"sentiment\"].value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neutral       8638\n",
              "worry         8459\n",
              "happiness     5209\n",
              "sadness       5165\n",
              "love          3842\n",
              "surprise      2187\n",
              "fun           1776\n",
              "relief        1526\n",
              "hate          1323\n",
              "empty          827\n",
              "enthusiasm     759\n",
              "boredom        179\n",
              "anger          110\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fleAL35GVZI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "VOCAB_SIZE = 50000\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(max_features=VOCAB_SIZE)\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "X = tfidf_vec.fit_transform(emotion_df[\"content\"])\n",
        "y = label_encoder.fit_transform(emotion_df[\"sentiment\"])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.33, \n",
        "                                                    random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40rHWxSfopq3",
        "colab_type": "code",
        "outputId": "ae668a2a-eb3e-4ffc-b1bd-fa82a73a6f25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(X.shape)\n",
        "X[0:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 48212)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<10x48212 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 103 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D59VgPGco31I",
        "colab_type": "code",
        "outputId": "0f899741-94e7-4a66-b7a6-3e591e24dd10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y[0:10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2, 10, 10,  3,  8, 12, 10, 12, 10, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jWmwjqKl6au",
        "colab_type": "code",
        "outputId": "ee5d8196-f1cb-40fd-bf18-863545195f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bayes = MultinomialNB()\n",
        "bayes.fit(X_train, y_train)\n",
        "predictions = bayes.predict(X_test)\n",
        "precision_score(predictions, y_test, average=\"micro\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2802272727272727"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-nDuzlymbco",
        "colab_type": "code",
        "outputId": "fac89573-4dc4-4a6c-91a0-496c2cbf6883",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "classifiers = {\"sgd\": SGDClassifier(loss=\"hinge\"),\n",
        "               \"svm\": SVC(), \n",
        "               \"random_forest\": RandomForestClassifier()}\n",
        "\n",
        "for lbl, clf in classifiers.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    predictions = clf.predict(X_test)\n",
        "    print(lbl, precision_score(predictions, y_test, average=\"micro\"))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sgd 0.32681818181818184\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "svm 0.21863636363636363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "random_forest 0.2721969696969697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HRpzAbSpBAS",
        "colab_type": "text"
      },
      "source": [
        "**Checking what our model learned**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNGmcb8Kq8QP",
        "colab_type": "code",
        "outputId": "a59fbe4d-03b4-42e7-80aa-88c73e40752f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(tfidf_vec.vocabulary_)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48212"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlWWbNCtpEAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import eye\n",
        "\n",
        "d = eye(len(tfidf_vec.vocabulary_))\n",
        "word_pred = bayes.predict_proba(d)\n",
        "inverse_vocab = {idx: word for word, idx in tfidf_vec.vocabulary_.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veBY1Uu1t_gk",
        "colab_type": "code",
        "outputId": "d2b7dfef-66a4-4eb2-8eee-9be1ef63b444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "word_pred.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48212, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wAQWdb8r4Jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter, defaultdict\n",
        "\n",
        "by_cls = defaultdict(Counter)\n",
        "for word_idx, pred in enumerate(word_pred):\n",
        "    for class_idx, score in enumerate(pred):\n",
        "        cls = label_encoder.classes_[class_idx]\n",
        "        by_cls[cls][inverse_vocab[word_idx]] = score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAZ5pWFmvUvH",
        "colab_type": "code",
        "outputId": "2934b201-7f52-46dd-f151-3e3676e12db1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "for k in by_cls:\n",
        "    words = [x[0] for x in by_cls[k].most_common(5)]\n",
        "    print(k, ':', ' '.join(words))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anger : confuzzled fridaaaayyyyy aaaaaaaaaaa transtelecom filthy\n",
            "boredom : squeaking ouuut cleanin sooooooo candyland3\n",
            "empty : _cheshire_cat_ bethsybsb conversating kimbermuffin less_than_3\n",
            "enthusiasm : lena_distractia foolproofdiva attending krisswouldhowse tatt\n",
            "fun : xbox bamboozle sanctuary oldies toodaayy\n",
            "happiness : excited woohoo excellent yay wars\n",
            "hate : hate hates suck fucking zomberellamcfox\n",
            "love : love mothers mommies moms loved\n",
            "neutral : www painting souljaboytellem link frenchieb\n",
            "relief : finally relax mastered relief inspiration\n",
            "sadness : sad sadly cry cried miss\n",
            "surprise : surprise wow surprised wtf surprisingly\n",
            "worry : worried poor throat hurts sick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkXAYanuRgyv",
        "colab_type": "text"
      },
      "source": [
        "**Training a deep model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OryVSH6svUfQ",
        "colab_type": "code",
        "outputId": "75c5d304-45f3-4fe6-f37b-86137c8c6230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from itertools import chain\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "chars = list(sorted(set(chain(*emotion_df[\"content\"]))))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "max_sequence_len = max(len(x) for x in emotion_df[\"content\"])\n",
        "\n",
        "char_vectors = []\n",
        "for txt in emotion_df[\"content\"]:\n",
        "    vec = np.zeros((max_sequence_len, len(char_to_idx)))\n",
        "    vec[np.arange(len(txt)), [char_to_idx[ch] for ch in txt]] = 1\n",
        "    char_vectors.append(vec)\n",
        "    \n",
        "char_vectors = np.asarray(char_vectors)\n",
        "char_vectors = pad_sequences(char_vectors)\n",
        "labels = label_encoder.transform(emotion_df[\"sentiment\"])\n",
        "\n",
        "def split(lst):\n",
        "    training_count = int(0.9*len(char_vectors))\n",
        "    return lst[:training_count], lst[training_count:]\n",
        "\n",
        "training_char_vectors, test_char_vectors = split(char_vectors)\n",
        "training_labels, test_labels = split(labels)\n",
        "\n",
        "char_vectors.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 167, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtI9_DJgatlt",
        "colab_type": "code",
        "outputId": "a9606c1d-58b5-4b61-9f77-0f8c6cf718d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        }
      },
      "source": [
        "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\n",
        "from keras.models import Model\n",
        "from keras.layers import Concatenate\n",
        "from keras import regularizers\n",
        "\n",
        "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
        "    char_input = Input(shape=(max_sequence_len, num_chars), name=\"input\")\n",
        "    conv_1x = Conv1D(128, 6, activation=\"relu\", padding=\"valid\")(char_input)\n",
        "    max_pool_1x = MaxPooling1D(6)(conv_1x)\n",
        "    conv_2x = Conv1D(256, 6, activation=\"relu\", padding=\"valid\")(max_pool_1x)\n",
        "    max_pool_2x = MaxPooling1D(6)(conv_2x)\n",
        "    \n",
        "    flatten = Flatten()(max_pool_2x)\n",
        "    dense = Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(flatten)\n",
        "    preds = Dense(num_labels, activation=\"softmax\")(dense)\n",
        "    \n",
        "    model = Model(char_input, preds)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
        "    return model\n",
        "\n",
        "char_cnn_model = create_char_cnn_model(len(char_to_idx), char_vectors.shape[1], len(label_encoder.classes_))\n",
        "char_cnn_model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0628 23:32:35.485127 139674020661120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0628 23:32:35.527176 139674020661120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0628 23:32:35.529573 139674020661120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0628 23:32:35.563861 139674020661120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0628 23:32:35.637488 139674020661120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0628 23:32:35.655550 139674020661120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 167, 100)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 162, 128)          76928     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 27, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 22, 256)           196864    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 3, 256)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               98432     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 13)                1677      \n",
            "=================================================================\n",
            "Total params: 373,901\n",
            "Trainable params: 373,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QntkZbhry3R6",
        "colab_type": "code",
        "outputId": "e1683c61-58bc-43d7-9063-4a2acaa31927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        }
      },
      "source": [
        "char_cnn_model.fit(training_char_vectors, training_labels, epochs=20, batch_size=1024)\n",
        "char_cnn_model.evaluate(test_char_vectors, test_labels)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0628 23:32:35.784398 139674020661120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0628 23:32:35.878913 139674020661120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "36000/36000 [==============================] - 9s 263us/step - loss: 3.2572 - acc: 0.2331\n",
            "Epoch 2/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 2.3862 - acc: 0.2481\n",
            "Epoch 3/20\n",
            "36000/36000 [==============================] - 3s 89us/step - loss: 2.2001 - acc: 0.2479\n",
            "Epoch 4/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 2.1541 - acc: 0.2508\n",
            "Epoch 5/20\n",
            "36000/36000 [==============================] - 3s 89us/step - loss: 2.1257 - acc: 0.2634\n",
            "Epoch 6/20\n",
            "36000/36000 [==============================] - 3s 89us/step - loss: 2.0960 - acc: 0.2762\n",
            "Epoch 7/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 2.0749 - acc: 0.2807\n",
            "Epoch 8/20\n",
            "36000/36000 [==============================] - 3s 89us/step - loss: 2.0599 - acc: 0.2909\n",
            "Epoch 9/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 2.0388 - acc: 0.2980\n",
            "Epoch 10/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 2.0287 - acc: 0.3036\n",
            "Epoch 11/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 2.0113 - acc: 0.3096\n",
            "Epoch 12/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 2.0023 - acc: 0.3126\n",
            "Epoch 13/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 1.9901 - acc: 0.3202\n",
            "Epoch 14/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 1.9809 - acc: 0.3244\n",
            "Epoch 15/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 1.9656 - acc: 0.3309\n",
            "Epoch 16/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 1.9550 - acc: 0.3364\n",
            "Epoch 17/20\n",
            "36000/36000 [==============================] - 3s 91us/step - loss: 1.9501 - acc: 0.3394\n",
            "Epoch 18/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 1.9395 - acc: 0.3427\n",
            "Epoch 19/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 1.9266 - acc: 0.3466\n",
            "Epoch 20/20\n",
            "36000/36000 [==============================] - 3s 90us/step - loss: 1.9170 - acc: 0.3510\n",
            "4000/4000 [==============================] - 0s 123us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.053533175468445, 0.31175]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNzooER4zn9Z",
        "colab_type": "code",
        "outputId": "6ce75e46-b975-4f1a-8559-158ba2dd3057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\n",
        "from keras.models import Model\n",
        "from keras.layers import Concatenate\n",
        "from keras import regularizers\n",
        "\n",
        "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
        "    char_input = Input(shape=(max_sequence_len, num_chars), name=\"input\")\n",
        "    \n",
        "    layers = []\n",
        "    \n",
        "    for window in (5, 6, 7):\n",
        "        conv_1x = Conv1D(128, window, activation=\"relu\", padding=\"valid\")(char_input)\n",
        "        max_pool_1x = MaxPooling1D(window)(conv_1x)\n",
        "        dropout_1x = Dropout(0.3)(max_pool_1x)\n",
        "        conv_2x = Conv1D(128, window, activation=\"relu\", padding=\"valid\")(dropout_1x)\n",
        "        max_pool_2x = MaxPooling1D(window)(conv_2x)\n",
        "        dropout_2x = Dropout(0.3)(max_pool_2x)\n",
        "        layers.append(dropout_2x)\n",
        "        \n",
        "    if len(layers) > 1:\n",
        "        merged = Concatenate(axis=1)(layers)\n",
        "    else:\n",
        "        merged = layers[0]\n",
        "        \n",
        "    dropout = Dropout(0.3)(merged)\n",
        "    \n",
        "    flatten = Flatten()(dropout)\n",
        "    dense = Dense(128, activation=\"relu\")(flatten)\n",
        "    preds = Dense(num_labels, activation=\"softmax\")(dense)\n",
        "    model = Model(char_input, preds)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='rmsprop',  metrics=['acc'])\n",
        "    return model\n",
        "\n",
        "char_cnn_model = create_char_cnn_model(len(char_to_idx), char_vectors.shape[1], len(label_encoder.classes_))\n",
        "char_cnn_model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0628 23:33:47.505040 139674020661120 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              (None, 167, 100)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 163, 128)     64128       input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 162, 128)     76928       input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 161, 128)     89728       input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 32, 128)      0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 27, 128)      0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1D)  (None, 23, 128)      0           conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 128)      0           max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 27, 128)      0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 23, 128)      0           max_pooling1d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 28, 128)      82048       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 22, 128)      98432       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 17, 128)      114816      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 5, 128)       0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 3, 128)       0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1D)  (None, 2, 128)       0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 5, 128)       0           max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 3, 128)       0           max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 2, 128)       0           max_pooling1d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 10, 128)      0           dropout_2[0][0]                  \n",
            "                                                                 dropout_4[0][0]                  \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 10, 128)      0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 1280)         0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          163968      flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 13)           1677        dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 691,725\n",
            "Trainable params: 691,725\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou3eeshMznkb",
        "colab_type": "code",
        "outputId": "6940f50f-e0ea-49a9-e0cd-a7ea4affa87f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "source": [
        "char_cnn_model.fit(training_char_vectors, training_labels, epochs=20, batch_size=1024)\n",
        "char_cnn_model.evaluate(test_char_vectors, test_labels)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "36000/36000 [==============================] - 7s 198us/step - loss: 2.1949 - acc: 0.2340\n",
            "Epoch 2/20\n",
            "36000/36000 [==============================] - 5s 147us/step - loss: 2.1344 - acc: 0.2458\n",
            "Epoch 3/20\n",
            "36000/36000 [==============================] - 5s 148us/step - loss: 2.1204 - acc: 0.2503\n",
            "Epoch 4/20\n",
            "36000/36000 [==============================] - 5s 148us/step - loss: 2.1031 - acc: 0.2586\n",
            "Epoch 5/20\n",
            "36000/36000 [==============================] - 5s 148us/step - loss: 2.0772 - acc: 0.2717\n",
            "Epoch 6/20\n",
            "36000/36000 [==============================] - 5s 147us/step - loss: 2.0452 - acc: 0.2891\n",
            "Epoch 7/20\n",
            "36000/36000 [==============================] - 5s 146us/step - loss: 2.0247 - acc: 0.2967\n",
            "Epoch 8/20\n",
            "36000/36000 [==============================] - 5s 147us/step - loss: 1.9984 - acc: 0.3070\n",
            "Epoch 9/20\n",
            "36000/36000 [==============================] - 5s 146us/step - loss: 1.9757 - acc: 0.3205\n",
            "Epoch 10/20\n",
            "36000/36000 [==============================] - 5s 146us/step - loss: 1.9576 - acc: 0.3254\n",
            "Epoch 11/20\n",
            "36000/36000 [==============================] - 5s 145us/step - loss: 1.9287 - acc: 0.3394\n",
            "Epoch 12/20\n",
            "36000/36000 [==============================] - 5s 145us/step - loss: 1.9174 - acc: 0.3436\n",
            "Epoch 13/20\n",
            "36000/36000 [==============================] - 5s 145us/step - loss: 1.8909 - acc: 0.3550\n",
            "Epoch 14/20\n",
            "36000/36000 [==============================] - 5s 146us/step - loss: 1.8822 - acc: 0.3595\n",
            "Epoch 15/20\n",
            "36000/36000 [==============================] - 5s 146us/step - loss: 1.8526 - acc: 0.3674\n",
            "Epoch 16/20\n",
            "36000/36000 [==============================] - 5s 147us/step - loss: 1.8325 - acc: 0.3759\n",
            "Epoch 17/20\n",
            "36000/36000 [==============================] - 5s 146us/step - loss: 1.8168 - acc: 0.3800\n",
            "Epoch 18/20\n",
            "36000/36000 [==============================] - 5s 146us/step - loss: 1.7934 - acc: 0.3910\n",
            "Epoch 19/20\n",
            "36000/36000 [==============================] - 5s 147us/step - loss: 1.7699 - acc: 0.3974\n",
            "Epoch 20/20\n",
            "36000/36000 [==============================] - 5s 147us/step - loss: 1.7569 - acc: 0.4031\n",
            "4000/4000 [==============================] - 1s 164us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.9519147100448608, 0.3565]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxn1ju2_EEk8",
        "colab_type": "text"
      },
      "source": [
        "**Featurizing and preparing our data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUbA9ZyCEDcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import one_hot\n",
        "\n",
        "VOCAB_SIZE = 50000\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE) # a dict key = word value= count  maxz-index = num_w\n",
        "tokenizer.fit_on_texts(emotion_df[\"content\"]) # "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNdu1eExJUlr",
        "colab_type": "code",
        "outputId": "57e805d3-e0fa-45ba-b845-01b2e5e7d6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# This may take a while to load\n",
        "\n",
        "w2v, idf = nb_utils.load_w2v(tokenizer)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVOH8R4MhjkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = tokenizer.texts_to_sequences(emotion_df[\"content\"]) # return matrix shape [len(emotion_df[\"content\"]), len(emotion_df[\"content\"][i])]\n",
        "tokens = pad_sequences(tokens)\n",
        "training_count = int(0.9*len(tokens))\n",
        "training_tokens, training_labels = tokens[:training_count], labels[:training_count]\n",
        "test_tokens, test_labels = tokens[training_count:], labels[training_count:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLAFsIFtl0gi",
        "colab_type": "code",
        "outputId": "17fb0acf-2d7c-4a22-f432-63d5d2a2acca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "source": [
        "from keras import layers, models\n",
        "import keras.backend as K\n",
        "\n",
        "def make_embedding(name, vocab_size, embedding_size, weights=None, mask_zero=True):\n",
        "    if weights is not None:\n",
        "        return layers.Embedding(mask_zero=mask_zero,\n",
        "                                input_dim=vocab_size, \n",
        "                                output_dim=weights.shape[1],\n",
        "                                weights=[weights],\n",
        "                                trainable=False,\n",
        "                                name=\"%s/embedding\" % name)\n",
        "    else:\n",
        "        return layers.Embedding(mask_zero=mask_zero,\n",
        "                                input_dim=vocab_size, \n",
        "                                output_dim=embedding_size,\n",
        "                                name=\"%s/embedding\" % name)\n",
        "    \n",
        "def create_unigram_model(vocab_size, embedding_size=None, embedding_weights=None, idf_weights=None):\n",
        "    assert not (embedding_size is None and embedding_weights is None)\n",
        "    message = layers.Input(shape=(None,), dtype=\"int32\", name=\"message\")\n",
        "    \n",
        "    embedding = make_embedding(\"message_vec\", vocab_size, embedding_size, embedding_weights)\n",
        "    idf = make_embedding(\"message_idf\", vocab_size, embedding_size, idf_weights)\n",
        "    mask = layers.Masking(mask_value=0)\n",
        "    \n",
        "    def _combine_and_sum(args):\n",
        "        embedding, idf = args\n",
        "        return K.sum(embedding*K.abs(idf), axis=1) # 实际张量是（None，None, 300)*（None，None, 1)\n",
        "    \n",
        "    sum_layer = layers.Lambda(_combine_and_sum, name=\"combine_and_sum\")\n",
        "    sum_msg = sum_layer([mask(embedding(message)), idf(message)])\n",
        "    fcl = layers.Dense(units=128, activation=\"relu\")(sum_msg)\n",
        "    categories = layers.Dense(units=len(label_encoder.classes_), activation=\"softmax\")(fcl)\n",
        "    \n",
        "    model = models.Model(inputs=[message], outputs=categories,)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "unigram_model = create_unigram_model(vocab_size=VOCAB_SIZE,\n",
        "                                     embedding_weights=w2v,\n",
        "                                     idf_weights=idf)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "message (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "message_vec/embedding (Embeddin (None, None, 300)    15000000    message[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "masking_1 (Masking)             (None, None, 300)    0           message_vec/embedding[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "message_idf/embedding (Embeddin (None, None, 1)      50000       message[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "combine_and_sum (Lambda)        (None, 300)          0           masking_1[0][0]                  \n",
            "                                                                 message_idf/embedding[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 128)          38528       combine_and_sum[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 13)           1677        dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 15,090,205\n",
            "Trainable params: 40,205\n",
            "Non-trainable params: 15,050,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPvGdXqw7JbP",
        "colab_type": "code",
        "outputId": "ae2a9cc8-efcc-4b3a-d8a0-b05f9567fc62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "unigram_model.fit(training_tokens, training_labels, epochs=10)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "36000/36000 [==============================] - 3s 79us/step - loss: 2.2767 - acc: 0.2890\n",
            "Epoch 2/10\n",
            "36000/36000 [==============================] - 3s 72us/step - loss: 1.9911 - acc: 0.3339\n",
            "Epoch 3/10\n",
            "36000/36000 [==============================] - 3s 72us/step - loss: 1.9481 - acc: 0.3438\n",
            "Epoch 4/10\n",
            "36000/36000 [==============================] - 3s 72us/step - loss: 1.9243 - acc: 0.3520\n",
            "Epoch 5/10\n",
            "36000/36000 [==============================] - 3s 72us/step - loss: 1.8996 - acc: 0.3604\n",
            "Epoch 6/10\n",
            "36000/36000 [==============================] - 3s 72us/step - loss: 1.8793 - acc: 0.3650\n",
            "Epoch 7/10\n",
            "36000/36000 [==============================] - 3s 71us/step - loss: 1.8616 - acc: 0.3681\n",
            "Epoch 8/10\n",
            "36000/36000 [==============================] - 3s 72us/step - loss: 1.8478 - acc: 0.3726\n",
            "Epoch 9/10\n",
            "36000/36000 [==============================] - 3s 71us/step - loss: 1.8312 - acc: 0.3813\n",
            "Epoch 10/10\n",
            "36000/36000 [==============================] - 3s 72us/step - loss: 1.8197 - acc: 0.3852\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f080cc6b710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4iKMA9g8Ot_",
        "colab_type": "code",
        "outputId": "a1dee8d1-030f-4fca-ac82-87583fdf642f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "unigram_model.evaluate(test_tokens, test_labels, verbose=2)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.382763785362244, 0.32375]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQz9tAU_8cgb",
        "colab_type": "text"
      },
      "source": [
        "**Learning Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiCvrplT8fcK",
        "colab_type": "code",
        "outputId": "d335d255-1a82-4da5-b34c-0af8f04f53bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "source": [
        "learned_embeddings_model = create_unigram_model(vocab_size=VOCAB_SIZE, embedding_size=25)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "message (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "message_vec/embedding (Embeddin (None, None, 25)     1250000     message[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "masking_2 (Masking)             (None, None, 25)     0           message_vec/embedding[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "message_idf/embedding (Embeddin (None, None, 25)     1250000     message[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "combine_and_sum (Lambda)        (None, 25)           0           masking_2[0][0]                  \n",
            "                                                                 message_idf/embedding[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 128)          3328        combine_and_sum[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 13)           1677        dense_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,505,005\n",
            "Trainable params: 2,505,005\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MBg1G3g86s-",
        "colab_type": "code",
        "outputId": "4c5465ec-0d12-4433-bb8c-41e1153149c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "learned_embeddings_model.fit(training_tokens, training_labels, epochs=10, batch_size=128)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "36000/36000 [==============================] - 2s 43us/step - loss: 2.1511 - acc: 0.2478\n",
            "Epoch 2/10\n",
            "36000/36000 [==============================] - 1s 33us/step - loss: 1.9898 - acc: 0.3157\n",
            "Epoch 3/10\n",
            "36000/36000 [==============================] - 1s 33us/step - loss: 1.8894 - acc: 0.3573\n",
            "Epoch 4/10\n",
            "36000/36000 [==============================] - 1s 33us/step - loss: 1.8247 - acc: 0.3822\n",
            "Epoch 5/10\n",
            "36000/36000 [==============================] - 1s 33us/step - loss: 1.7721 - acc: 0.4038\n",
            "Epoch 6/10\n",
            "36000/36000 [==============================] - 1s 32us/step - loss: 1.7216 - acc: 0.4212\n",
            "Epoch 7/10\n",
            "36000/36000 [==============================] - 1s 33us/step - loss: 1.6706 - acc: 0.4405\n",
            "Epoch 8/10\n",
            "36000/36000 [==============================] - 1s 32us/step - loss: 1.6174 - acc: 0.4580\n",
            "Epoch 9/10\n",
            "36000/36000 [==============================] - 1s 32us/step - loss: 1.5616 - acc: 0.4809\n",
            "Epoch 10/10\n",
            "36000/36000 [==============================] - 1s 32us/step - loss: 1.5044 - acc: 0.5026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f07b00a7eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWDaEPU89X3U",
        "colab_type": "code",
        "outputId": "ccd4345e-b7ec-4577-e703-9d3fc81575cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Note the test set accuracy is lower than that on the training set.\n",
        "\n",
        "learned_embeddings_model.evaluate(test_tokens, test_labels, verbose=2)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.9881738328933716, 0.362]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyNUz3Ixa64P",
        "colab_type": "text"
      },
      "source": [
        "**More Complex Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-iHOea_dAw0",
        "colab_type": "text"
      },
      "source": [
        "As with our previous task, we can try using more powerful models to classify our text. In this case, the limited training data and text size limit their effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVfnoFqyayL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_cnn_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
        "    message = layers.Input(shape=(None, ), dtype=\"int32\", name=\"title\")\n",
        "    # The convolution layer in keras does not support masking, so we just allow\n",
        "    # the embedding layer to learn an explicit value.\n",
        "    embedding = make_embedding(\"message_vec\", vocab_size, \n",
        "                               embedding_size, embedding_weights, \n",
        "                               mask_zero=False)\n",
        "    def _combine_sum(v):\n",
        "        return K.sum(v, axis=1)\n",
        "    \n",
        "    cnn_1 = layers.Convolution1D(128, 3)\n",
        "    cnn_2 = layers.Convolution1D(128, 3)\n",
        "    cnn_3 = layers.Convolution1D(128, 3)\n",
        "    \n",
        "    global_pool = layers.GlobalMaxPooling1D()\n",
        "    local_pool = layers.MaxPooling1D(strides=1, pool_size=3)\n",
        "    \n",
        "    cnn_encoding = global_pool(cnn_3(local_pool(cnn_2(local_pool(cnn_1(embedding(message)))))))\n",
        "                                 \n",
        "\n",
        "    fcl = layers.Dense(units=128, activation=\"elu\")(cnn_encoding)\n",
        "    categories = layers.Dense(units=len(label_encoder.classes_), activation=\"softmax\")(fcl)\n",
        "    model = models.Model(\n",
        "        inputs=[message],\n",
        "        outputs=[categories],\n",
        "    )\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    return model\n",
        "                               \n",
        "                               "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9czv2PFCp_CT",
        "colab_type": "code",
        "outputId": "48735f51-7e37-4a31-b3e8-cd918332249e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "cnn_model = create_cnn_model(VOCAB_SIZE, embedding_weights=w2v)\n",
        "cnn_model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "title (InputLayer)              (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "message_vec/embedding (Embeddin (None, None, 300)    15000000    title[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, None, 128)    115328      message_vec/embedding[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_9 (MaxPooling1D)  (None, None, 128)    0           conv1d_9[0][0]                   \n",
            "                                                                 conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, None, 128)    49280       max_pooling1d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, None, 128)    49280       max_pooling1d_9[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          16512       global_max_pooling1d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 13)           1677        dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 15,232,077\n",
            "Trainable params: 232,077\n",
            "Non-trainable params: 15,000,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k34c0PVHp7B4",
        "colab_type": "code",
        "outputId": "bd3dcc23-db93-481f-b22f-3c9b7548c1e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "cnn_model.fit(training_tokens, training_labels, epochs=10)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "36000/36000 [==============================] - 5s 141us/step - loss: 1.9761 - acc: 0.3179\n",
            "Epoch 2/10\n",
            "36000/36000 [==============================] - 5s 126us/step - loss: 1.8614 - acc: 0.3568\n",
            "Epoch 3/10\n",
            "36000/36000 [==============================] - 5s 127us/step - loss: 1.7953 - acc: 0.3813\n",
            "Epoch 4/10\n",
            "36000/36000 [==============================] - 5s 125us/step - loss: 1.7177 - acc: 0.4044\n",
            "Epoch 5/10\n",
            "36000/36000 [==============================] - 5s 125us/step - loss: 1.6250 - acc: 0.4370\n",
            "Epoch 6/10\n",
            "36000/36000 [==============================] - 4s 124us/step - loss: 1.5112 - acc: 0.4784\n",
            "Epoch 7/10\n",
            "36000/36000 [==============================] - 5s 125us/step - loss: 1.3913 - acc: 0.5164\n",
            "Epoch 8/10\n",
            "36000/36000 [==============================] - 4s 124us/step - loss: 1.2722 - acc: 0.5559\n",
            "Epoch 9/10\n",
            "36000/36000 [==============================] - 5s 125us/step - loss: 1.1634 - acc: 0.5955\n",
            "Epoch 10/10\n",
            "36000/36000 [==============================] - 5s 131us/step - loss: 1.0625 - acc: 0.6305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f07b0b124e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPqgCXFSq5uL",
        "colab_type": "code",
        "outputId": "eeedccb3-b4c4-4ca4-961e-71ef33c2fcbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "cnn_model.evaluate(test_tokens, test_labels)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4000/4000 [==============================] - 0s 85us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.6002593450546265, 0.31425]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbD0Goy6q9cJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_lstm_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
        "    message = layers.Input(shape=(None,), dtype='int32', name='title')\n",
        "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights)(message)\n",
        "#     mask = layers.Masking(mask_value=0)(embedding)\n",
        "    lstm_1 = layers.LSTM(units=128, return_sequences=False)(embedding)\n",
        "#     lstm_2 = layers.LSTM(units=128, return_sequences=False)(lstm_1)\n",
        "    category = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(lstm_1)\n",
        "    \n",
        "    model = models.Model(\n",
        "        inputs=[message],\n",
        "        outputs=[category],\n",
        "    )\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibYOX7uxu3Ip",
        "colab_type": "code",
        "outputId": "a1b885d6-3a02-4ce0-dc95-d94fc3392b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "lstm_model = create_lstm_model(VOCAB_SIZE, embedding_weights=w2v)\n",
        "lstm_model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "title (InputLayer)           (None, None)              0         \n",
            "_________________________________________________________________\n",
            "message_vec/embedding (Embed (None, None, 300)         15000000  \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               219648    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 13)                1677      \n",
            "=================================================================\n",
            "Total params: 15,221,325\n",
            "Trainable params: 221,325\n",
            "Non-trainable params: 15,000,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ1mdDYfxoD9",
        "colab_type": "code",
        "outputId": "dae82377-8d1e-4375-d286-a5018412a3a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "lstm_model.fit(training_tokens, training_labels, epochs=10, batch_size=128)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "36000/36000 [==============================] - 20s 548us/step - loss: 2.0207 - acc: 0.3104\n",
            "Epoch 2/10\n",
            "36000/36000 [==============================] - 19s 525us/step - loss: 1.8928 - acc: 0.3529\n",
            "Epoch 3/10\n",
            "36000/36000 [==============================] - 19s 525us/step - loss: 1.8485 - acc: 0.3669\n",
            "Epoch 4/10\n",
            "36000/36000 [==============================] - 19s 526us/step - loss: 1.8171 - acc: 0.3754\n",
            "Epoch 5/10\n",
            "36000/36000 [==============================] - 19s 524us/step - loss: 1.7913 - acc: 0.3849\n",
            "Epoch 6/10\n",
            "36000/36000 [==============================] - 19s 524us/step - loss: 1.7670 - acc: 0.3899\n",
            "Epoch 7/10\n",
            "36000/36000 [==============================] - 19s 526us/step - loss: 1.7427 - acc: 0.4007\n",
            "Epoch 8/10\n",
            "36000/36000 [==============================] - 19s 521us/step - loss: 1.7188 - acc: 0.4079\n",
            "Epoch 9/10\n",
            "36000/36000 [==============================] - 19s 532us/step - loss: 1.6910 - acc: 0.4163\n",
            "Epoch 10/10\n",
            "36000/36000 [==============================] - 19s 526us/step - loss: 1.6612 - acc: 0.4254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0661dd5278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr2P3ZmY0MfK",
        "colab_type": "code",
        "outputId": "9a0bcb23-7403-488c-8224-a0dcdbd1c82b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "lstm_model.evaluate(test_tokens, test_labels)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4000/4000 [==============================] - 3s 658us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.9042014083862304, 0.38075]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HolOoreA0dVW",
        "colab_type": "text"
      },
      "source": [
        "**Comparing our models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kll27TiP0cl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = {\"lstm\": lstm_model.predict(test_tokens[:100]),\n",
        "               \"char_cnn\": char_cnn_model.predict(test_char_vectors[:100]), \n",
        "               \"cnn\": cnn_model.predict(test_tokens[:100]), \n",
        "               \"unigram\": unigram_model.predict(test_tokens[:100])}\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcANjZUT0Ub0",
        "colab_type": "code",
        "outputId": "00d0ab0f-f08d-426f-ea14-bf40033fa0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "# Make a dataframe just for test data\n",
        "\n",
        "pd.options.display.max_colwidth = 128\n",
        "test_df = emotion_df[training_count:training_count+100].reset_index()\n",
        "eval_df = pd.DataFrame({\n",
        "    'content': test_df['content'],\n",
        "    'true': test_df['sentiment'],\n",
        "    'lstm': [label_encoder.classes_[np.argmax(x)] for x in predictions['lstm']],\n",
        "    'cnn': [label_encoder.classes_[np.argmax(x)] for x in predictions['cnn']],\n",
        "    'char_cnn': [label_encoder.classes_[np.argmax(x)] for x in predictions['char_cnn']],    \n",
        "    'unigram': [label_encoder.classes_[np.argmax(x)] for x in predictions['unigram']],\n",
        "})\n",
        "eval_df = eval_df[['content', 'true', 'lstm', 'cnn', 'char_cnn', 'unigram']]\n",
        "eval_df.head(10)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>true</th>\n",
              "      <th>lstm</th>\n",
              "      <th>cnn</th>\n",
              "      <th>char_cnn</th>\n",
              "      <th>unigram</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HAPPY MOTHER'S DAY to all of the wonderful women out there.  Have a great and relaxful day.</td>\n",
              "      <td>happiness</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>browsing thru adopting agencies, i'm gonna get some exotic kids</td>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...</td>\n",
              "      <td>love</td>\n",
              "      <td>worry</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Happy Mother's Day to all the Mommiessss</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@mattgarner haha what's up Matt ?</td>\n",
              "      <td>happiness</td>\n",
              "      <td>neutral</td>\n",
              "      <td>happiness</td>\n",
              "      <td>neutral</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What's up!!? @guillermop</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>worry</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>@KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.</td>\n",
              "      <td>fun</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>@TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp;amp; my 'joper' w/their Dad...</td>\n",
              "      <td>happiness</td>\n",
              "      <td>neutral</td>\n",
              "      <td>worry</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>@nak1a &amp;quot;If there's a camel up a hill&amp;quot; and &amp;quot;I'll give you plankton&amp;quot; ....HILARIOUS!!</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "      <td>love</td>\n",
              "      <td>happiness</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>@Bern_morley LOL I love your kids</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                           content  ...    unigram\n",
              "0                                      HAPPY MOTHER'S DAY to all of the wonderful women out there.  Have a great and relaxful day.  ...       love\n",
              "1                                                                  browsing thru adopting agencies, i'm gonna get some exotic kids  ...  happiness\n",
              "2  I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...  ...  happiness\n",
              "3                                                                                         Happy Mother's Day to all the Mommiessss  ...       love\n",
              "4                                                                                                @mattgarner haha what's up Matt ?  ...  happiness\n",
              "5                                                                                                         What's up!!? @guillermop  ...    neutral\n",
              "6                                       @KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.  ...  happiness\n",
              "7  @TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp; my 'joper' w/their Dad...  ...  happiness\n",
              "8                           @nak1a &quot;If there's a camel up a hill&quot; and &quot;I'll give you plankton&quot; ....HILARIOUS!!  ...    neutral\n",
              "9                                                                                                @Bern_morley LOL I love your kids  ...  happiness\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nLL5AUe7M5V",
        "colab_type": "text"
      },
      "source": [
        "**Qualitative Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q41tU75t7MKb",
        "colab_type": "code",
        "outputId": "1398bc7c-93d6-48c2-c981-ce9c4a773eca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "eval_df[eval_df['lstm'] != eval_df['true']].head(10)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>true</th>\n",
              "      <th>lstm</th>\n",
              "      <th>cnn</th>\n",
              "      <th>char_cnn</th>\n",
              "      <th>unigram</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HAPPY MOTHER'S DAY to all of the wonderful women out there.  Have a great and relaxful day.</td>\n",
              "      <td>happiness</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>browsing thru adopting agencies, i'm gonna get some exotic kids</td>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...</td>\n",
              "      <td>love</td>\n",
              "      <td>worry</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@mattgarner haha what's up Matt ?</td>\n",
              "      <td>happiness</td>\n",
              "      <td>neutral</td>\n",
              "      <td>happiness</td>\n",
              "      <td>neutral</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>@KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.</td>\n",
              "      <td>fun</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>@TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp;amp; my 'joper' w/their Dad...</td>\n",
              "      <td>happiness</td>\n",
              "      <td>neutral</td>\n",
              "      <td>worry</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>@davecandoit dude that honest to god happens to me all the time.. minus the trail mix.</td>\n",
              "      <td>sadness</td>\n",
              "      <td>neutral</td>\n",
              "      <td>worry</td>\n",
              "      <td>worry</td>\n",
              "      <td>worry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Happy Mother's Day to the tweetin' mamas  Nite tweeple!</td>\n",
              "      <td>worry</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>On my way home...then SLEEP! Seeing Amber Pacific tomorow with the besties</td>\n",
              "      <td>happiness</td>\n",
              "      <td>neutral</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>@xoMusicLoverxo I'm using it in a story. I actually already wrote it but have to write the chapters before it.</td>\n",
              "      <td>relief</td>\n",
              "      <td>neutral</td>\n",
              "      <td>surprise</td>\n",
              "      <td>love</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                            content  ...    unigram\n",
              "0                                       HAPPY MOTHER'S DAY to all of the wonderful women out there.  Have a great and relaxful day.  ...       love\n",
              "1                                                                   browsing thru adopting agencies, i'm gonna get some exotic kids  ...  happiness\n",
              "2   I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...  ...  happiness\n",
              "4                                                                                                 @mattgarner haha what's up Matt ?  ...  happiness\n",
              "6                                        @KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.  ...  happiness\n",
              "7   @TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp; my 'joper' w/their Dad...  ...  happiness\n",
              "10                                           @davecandoit dude that honest to god happens to me all the time.. minus the trail mix.  ...      worry\n",
              "12                                                                          Happy Mother's Day to the tweetin' mamas  Nite tweeple!  ...  happiness\n",
              "13                                                       On my way home...then SLEEP! Seeing Amber Pacific tomorow with the besties  ...  happiness\n",
              "14                   @xoMusicLoverxo I'm using it in a story. I actually already wrote it but have to write the chapters before it.  ...    neutral\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spHtffNQafaY",
        "colab_type": "text"
      },
      "source": [
        "**Analyzing Tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMp3Uy0vascR",
        "colab_type": "code",
        "outputId": "00f13cd5-5b5a-4bc4-9b86-26d4be86506d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install twitter"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: twitter in /usr/local/lib/python3.6/dist-packages (1.18.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhIpsTIoayoa",
        "colab_type": "code",
        "outputId": "040eb2d8-585f-464b-df15-0309008fa9d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73MTMd4Ckx_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import twitter\n",
        "import emoji\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from itertools import chain\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import one_hot\n",
        "import keras.callbacks\n",
        "import json\n",
        "\n",
        "import os\n",
        "import nb_utils\n",
        "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, Embedding, GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers import Concatenate, Average\n",
        "\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F376QyNkk-Ds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fill these in!\n",
        "\n",
        "CONSUMER_KEY = 'xbMuxcJpRTiVGt2C2EYnA'\n",
        "CONSUMER_SECRET = '2DbQTsvIptkPTdaUcos8DDvQH9fzO0hNjJpUT2uVzQ'\n",
        "ACCESS_TOKEN = '7319442-EDm4CPxL7W4KkZcGWRMJNVHp88W5OH9vgblu898fg'\n",
        "ACCESS_SECRET = '5ZxJSbqXhG7uhgXzTFWf9XhkfsxxinlPRXyDTzbA9w'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPrZyaqwlCjz",
        "colab_type": "code",
        "outputId": "2218056f-5084-4814-dd7a-a3243afa5480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "auth=twitter.OAuth(\n",
        "    consumer_key=CONSUMER_KEY,\n",
        "    consumer_secret=CONSUMER_SECRET,\n",
        "    token=ACCESS_TOKEN,\n",
        "    token_secret=ACCESS_SECRET,\n",
        ")\n",
        "\n",
        "status_stream = twitter.TwitterStream(auth=auth).statuses\n",
        "\n",
        "[x['text'] for x in itertools.islice(status_stream.sample(), 0, 5) if x.get('text')]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT @SLOWTOWNVHS: tried to unfollow the most annoying bitch on twitter but all it said was edit profile \\n\\n https://t.co/FZ2YWcbUwM',\n",
              " '@tanamongeau JDHDUDIBD',\n",
              " '@ActuallyDice UwU I would also like cuddles from you too']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIqODDRZlTVQ",
        "colab_type": "code",
        "outputId": "636aecdb-14db-4db5-dd68-114d81950999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "tatus_stream = twitter.TwitterStream(auth=auth).statuses\n",
        "\n",
        "def english_has_emoji(tweet):\n",
        "    if tweet.get('lang') != 'en':\n",
        "        return False\n",
        "    return any(ch for ch in tweet.get('text', '') if ch in emoji.UNICODE_EMOJI)\n",
        "\n",
        "%time tweets = list(itertools.islice(filter(english_has_emoji, status_stream.sample()), 0, 100))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.51 s, sys: 307 ms, total: 1.82 s\n",
            "Wall time: 55.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkK8_QBAmAyh",
        "colab_type": "code",
        "outputId": "5122779d-a9c1-4b43-cd78-7a99ec62e2ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "stripped = []\n",
        "for tweet in tweets:\n",
        "    text = tweet['text']\n",
        "    emojis = {ch for ch in text if ch in emoji.UNICODE_EMOJI}\n",
        "    if len(emojis) == 1:\n",
        "        emoiji = emojis.pop()\n",
        "        text = ''.join(ch for ch in text if ch != emoiji)\n",
        "        stripped.append((text, emoiji))\n",
        "len(stripped)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_QofSTW8Cx-",
        "colab_type": "text"
      },
      "source": [
        "**Using the CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nO8ImCg8Hlv",
        "colab_type": "code",
        "outputId": "8c049eb8-74fe-46f9-c148-4be27d8fda20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "all_tweets = pd.read_csv(\"data/emojis.csv\")\n",
        "all_tweets.head()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>emoji</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@ATLHawks: Chance The Rapper or Kent Bazemore? #Chance3 #ColoringBook #Twins</td>\n",
              "      <td>🤔</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@nice_aju: Yup we love you, you're so precious #WeLoveYouHoseok</td>\n",
              "      <td>💙</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Fav  Sing Me to Sleep by Alan Walker</td>\n",
              "      <td>💛</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@AshBenzo: Wife From The Real-Life 'Fault In Our Stars' Couple Dies 5 Days After Her Husband</td>\n",
              "      <td>💔</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Why am I up so late</td>\n",
              "      <td>😔</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                           text emoji\n",
              "0                  @ATLHawks: Chance The Rapper or Kent Bazemore? #Chance3 #ColoringBook #Twins     🤔\n",
              "1                               @nice_aju: Yup we love you, you're so precious #WeLoveYouHoseok     💙\n",
              "2                                                          Fav  Sing Me to Sleep by Alan Walker     💛\n",
              "3  @AshBenzo: Wife From The Real-Life 'Fault In Our Stars' Couple Dies 5 Days After Her Husband     💔\n",
              "4                                                                           Why am I up so late     😔"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwf5_61Z8gcj",
        "colab_type": "code",
        "outputId": "8cd9ffa9-e6e3-498a-c428-5d92112ef149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "all_tweets.info()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 806203 entries, 0 to 806202\n",
            "Data columns (total 2 columns):\n",
            "text     806203 non-null object\n",
            "emoji    806203 non-null object\n",
            "dtypes: object(2)\n",
            "memory usage: 12.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WENAXpqB8r8Z",
        "colab_type": "code",
        "outputId": "e2d363b5-66bf-4c61-fc43-e19556f1dbab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "all_tweets[\"emoji\"].value_counts()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "😂    124823\n",
              "❤     43218\n",
              "😍     40566\n",
              "😭     35714\n",
              "😊     20076\n",
              "🙄     17963\n",
              "😩     16232\n",
              "🔥     15453\n",
              "🤔     15419\n",
              "💕     12026\n",
              "💯     11783\n",
              "😘     11065\n",
              "💀      9928\n",
              "✨      9886\n",
              "🙃      9405\n",
              "👀      7842\n",
              "😒      7019\n",
              "☺      6871\n",
              "😢      6846\n",
              "😳      6716\n",
              "💙      6616\n",
              "😎      6349\n",
              "😉      6272\n",
              "😅      6133\n",
              "😁      6010\n",
              "😌      5759\n",
              "😏      5623\n",
              "💖      5331\n",
              "😔      5244\n",
              "😴      4999\n",
              "      ...  \n",
              "🏬         1\n",
              "🏤         1\n",
              "🤡         1\n",
              "🚸         1\n",
              "🏣         1\n",
              "㊙         1\n",
              "🔏         1\n",
              "🦈         1\n",
              "🏦         1\n",
              "📂         1\n",
              "🔀         1\n",
              "🕣         1\n",
              "📇         1\n",
              "🤠         1\n",
              "🕡         1\n",
              "↩         1\n",
              "📳         1\n",
              "🏗         1\n",
              "🈹         1\n",
              "👘         1\n",
              "🎑         1\n",
              "📭         1\n",
              "🔣         1\n",
              "🛐         1\n",
              "🕍         1\n",
              "🕜         1\n",
              "🈚         1\n",
              "🗂         1\n",
              "🥇         1\n",
              "🎚         1\n",
              "Name: emoji, Length: 989, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YBNhW3G9zXo",
        "colab_type": "code",
        "outputId": "d91b15bc-1887-409c-aaa9-84c615394d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "tweets = all_tweets.groupby(\"emoji\").filter(lambda c: len(c) > 1000)\n",
        "tweets.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>emoji</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@ATLHawks: Chance The Rapper or Kent Bazemore? #Chance3 #ColoringBook #Twins</td>\n",
              "      <td>🤔</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@nice_aju: Yup we love you, you're so precious #WeLoveYouHoseok</td>\n",
              "      <td>💙</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Fav  Sing Me to Sleep by Alan Walker</td>\n",
              "      <td>💛</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@AshBenzo: Wife From The Real-Life 'Fault In Our Stars' Couple Dies 5 Days After Her Husband</td>\n",
              "      <td>💔</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Why am I up so late</td>\n",
              "      <td>😔</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                           text emoji\n",
              "0                  @ATLHawks: Chance The Rapper or Kent Bazemore? #Chance3 #ColoringBook #Twins     🤔\n",
              "1                               @nice_aju: Yup we love you, you're so precious #WeLoveYouHoseok     💙\n",
              "2                                                          Fav  Sing Me to Sleep by Alan Walker     💛\n",
              "3  @AshBenzo: Wife From The Real-Life 'Fault In Our Stars' Couple Dies 5 Days After Her Husband     💔\n",
              "4                                                                           Why am I up so late     😔"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8Yf2wjp-qw9",
        "colab_type": "code",
        "outputId": "016872f8-fb9d-4400-c3df-cdc23835a343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tweets['emoji'].value_counts()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "😂    124823\n",
              "❤     43218\n",
              "😍     40566\n",
              "😭     35714\n",
              "😊     20076\n",
              "🙄     17963\n",
              "😩     16232\n",
              "🔥     15453\n",
              "🤔     15419\n",
              "💕     12026\n",
              "💯     11783\n",
              "😘     11065\n",
              "💀      9928\n",
              "✨      9886\n",
              "🙃      9405\n",
              "👀      7842\n",
              "😒      7019\n",
              "☺      6871\n",
              "😢      6846\n",
              "😳      6716\n",
              "💙      6616\n",
              "😎      6349\n",
              "😉      6272\n",
              "😅      6133\n",
              "😁      6010\n",
              "😌      5759\n",
              "😏      5623\n",
              "💖      5331\n",
              "😔      5244\n",
              "😴      4999\n",
              "      ...  \n",
              "✌      1523\n",
              "📸      1496\n",
              "🎤      1487\n",
              "🌚      1452\n",
              "👅      1431\n",
              "🏈      1373\n",
              "🌟      1355\n",
              "⏩      1332\n",
              "❗      1325\n",
              "🔴      1304\n",
              "☕      1296\n",
              "👊      1273\n",
              "👇      1259\n",
              "❣      1254\n",
              "🎧      1246\n",
              "🎈      1210\n",
              "⏭      1198\n",
              "💫      1181\n",
              "↪      1157\n",
              "🤑      1152\n",
              "⚽      1141\n",
              "😹      1112\n",
              "😶      1108\n",
              "💦      1075\n",
              "😣      1074\n",
              "😥      1072\n",
              "🙁      1066\n",
              "🤕      1065\n",
              "😰      1013\n",
              "☀      1013\n",
              "Name: emoji, Length: 121, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtCH5xJK-zEG",
        "colab_type": "code",
        "outputId": "48276ae6-8892-44d1-d9fc-f9302872f0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "max(tweets[\"text\"], key=lambda t: len(t))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Don't worry, my love, I don't get you wrong Don't get me wrong. We're connected with our hearts, souls, minds, bodies. If any doubt, we ask\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB8xGC76_Zf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = list(sorted(set(chain(*tweets['text']))))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "max_sequence_len = max(len(x) for x in tweets['text'])\n",
        "\n",
        "emojis = list(sorted(set(tweets['emoji'])))\n",
        "emoji_to_idx = {em: idx for idx, em in enumerate(emojis)}\n",
        "emojis[:10]\n",
        "\n",
        "train_tweets, test_tweets = train_test_split(tweets, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGSqDi0eOj83",
        "colab_type": "code",
        "outputId": "6881a48f-4e3c-47e8-8169-2bad6e2914a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(chars), len(emojis))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96 121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXF0KyNV_tO5",
        "colab_type": "code",
        "outputId": "199e3c1b-0f03-4e12-983a-685fe1a0a643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        }
      },
      "source": [
        "def data_generator(tweets, batch_size):\n",
        "    while True:\n",
        "        if batch_size is None:\n",
        "            batch = tweets\n",
        "            batch_size = batch.shape[0]\n",
        "        else:\n",
        "            batch = tweets.sample(batch_size)\n",
        "        X = np.zeros((batch_size, max_sequence_len, len(chars)))\n",
        "        y = np.zeros((batch_size,))\n",
        "        for row_idx, (_, row) in enumerate(batch.iterrows()):\n",
        "            y[row_idx] = emoji_to_idx[row[\"emoji\"]]\n",
        "            for ch_idx, ch in enumerate(row[\"text\"]):\n",
        "                X[row_idx, ch_idx, char_to_idx[ch]] = 1\n",
        "        yield X, y\n",
        "        \n",
        "next(data_generator(tweets, 10)) [0]      "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG0eLun3IeaB",
        "colab_type": "code",
        "outputId": "22947d08-ec1a-4b66-9971-f9a5de5c84fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "next(data_generator(tweets, 10))[0].shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 139, 96)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EWIuPLbGs4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
        "    char_input = Input(shape=(max_sequence_len, num_chars),name=\"char_cnn_input\")\n",
        "    conv_1x = Conv1D(128, 6, activation=\"relu\", padding=\"valid\")(char_input)\n",
        "    max_pool_1x = MaxPooling1D(4)(conv_1x)\n",
        "    conv_2x = Conv1D(256, 6, activation=\"relu\", padding=\"valid\")(max_pool_1x)\n",
        "    max_pool_2x = MaxPooling1D(4)(conv_2x)\n",
        "    \n",
        "    flatten = Flatten()(max_pool_2x)\n",
        "    dense = Dense(128, activation=\"relu\")(flatten)\n",
        "    preds = Dense(num_labels, activation=\"softmax\", name=\"char_cnn_preductins\")(dense)\n",
        "    \n",
        "    model = Model(char_input, preds)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAV9t-bQPTLy",
        "colab_type": "code",
        "outputId": "ebb3baa4-29b7-4ba1-f35d-e5b7cef32c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "source": [
        "char_cnn_model = create_char_cnn_model(len(char_to_idx), max_sequence_len, len(emojis))\n",
        "char_cnn_model.summary()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "char_cnn_input (InputLayer)  (None, 139, 96)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_12 (Conv1D)           (None, 134, 128)          73856     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_10 (MaxPooling (None, 33, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_13 (Conv1D)           (None, 28, 256)           196864    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_11 (MaxPooling (None, 7, 256)            0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 1792)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               229504    \n",
            "_________________________________________________________________\n",
            "char_cnn_preductins (Dense)  (None, 121)               15609     \n",
            "=================================================================\n",
            "Total params: 515,833\n",
            "Trainable params: 515,833\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_DgYQDCPA4T",
        "colab_type": "code",
        "outputId": "f615d031-6694-4a67-96f0-b87ca4f2145c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        }
      },
      "source": [
        "early = keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=0.03, \n",
        "                                      patience=2, verbose=0, mode=\"auto\")\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "char_cnn_model.fit_generator(data_generator(train_tweets, batch_size=BATCH_SIZE),\n",
        "                             epochs=20, \n",
        "                             steps_per_epoch=len(train_tweets)/BATCH_SIZE, \n",
        "                             verbose=2, callbacks=[early])\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            " - 147s - loss: 3.5995 - acc: 0.2206\n",
            "Epoch 2/20\n",
            " - 146s - loss: 3.2039 - acc: 0.2879\n",
            "Epoch 3/20\n",
            " - 144s - loss: 3.0022 - acc: 0.3277\n",
            "Epoch 4/20\n",
            " - 144s - loss: 2.8538 - acc: 0.3581\n",
            "Epoch 5/20\n",
            " - 144s - loss: 2.7429 - acc: 0.3802\n",
            "Epoch 6/20\n",
            " - 144s - loss: 2.6571 - acc: 0.3965\n",
            "Epoch 7/20\n",
            " - 145s - loss: 2.5914 - acc: 0.4093\n",
            "Epoch 8/20\n",
            " - 146s - loss: 2.5290 - acc: 0.4213\n",
            "Epoch 9/20\n",
            " - 146s - loss: 2.4795 - acc: 0.4296\n",
            "Epoch 10/20\n",
            " - 143s - loss: 2.4300 - acc: 0.4379\n",
            "Epoch 11/20\n",
            " - 144s - loss: 2.3969 - acc: 0.4435\n",
            "Epoch 12/20\n",
            " - 143s - loss: 2.3575 - acc: 0.4513\n",
            "Epoch 13/20\n",
            " - 144s - loss: 2.3283 - acc: 0.4569\n",
            "Epoch 14/20\n",
            " - 145s - loss: 2.2962 - acc: 0.4622\n",
            "Epoch 15/20\n",
            " - 147s - loss: 2.2752 - acc: 0.4647\n",
            "Epoch 16/20\n",
            " - 146s - loss: 2.2526 - acc: 0.4692\n",
            "Epoch 17/20\n",
            " - 145s - loss: 2.2359 - acc: 0.4711\n",
            "Epoch 18/20\n",
            " - 143s - loss: 2.2148 - acc: 0.4747\n",
            "Epoch 19/20\n",
            " - 143s - loss: 2.1995 - acc: 0.4771\n",
            "Epoch 20/20\n",
            " - 143s - loss: 2.1807 - acc: 0.4803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f07b0ad3198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdxBn29pVcla",
        "colab_type": "code",
        "outputId": "1b2ca81f-b97f-430b-93b3-523097d3b3fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "char_cnn_model.evaluate_generator(data_generator(test_tweets, batch_size=BATCH_SIZE), \n",
        "                                  steps=len(test_tweets) / BATCH_SIZE)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.3476517373237056, 0.35964390851449274]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbAj4P0cXSny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"zoo/07/emoji_chars.json\", \"w\") as fout:\n",
        "    json.dump({\"emojis\": \"\".join(emojis),\n",
        "               \"char_to_idx\": char_to_idx, \n",
        "               \"max_sequence_len\": max_sequence_len,},\n",
        "              fout)\n",
        "    \n",
        "char_cnn_model.save(\"zoo/07/char_cnn_model.h5\")\n",
        "char_cnn_model.save_weights(\"zoo/07/char_cnn_model_weights.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtb-1AgoY4HQ",
        "colab_type": "code",
        "outputId": "2fd3360e-1d7c-44ec-8664-27f674328255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "pd.options.display.max_colwidth = 128\n",
        "inspect_tweets = test_tweets.sample(100)\n",
        "predicted = char_cnn_model.predict_generator(data_generator(inspect_tweets, batch_size=None), steps=1)\n",
        "show = pd.DataFrame({\"text\": inspect_tweets[\"text\"], \n",
        "                     \"true\": inspect_tweets[\"emoji\"], \n",
        "                     \"pred\": [emojis[np.argmax(x)] for x in predicted],})\n",
        "show = show[[\"text\", \"true\", \"pred\"]]\n",
        "show.head()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>true</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>229070</th>\n",
              "      <td>@v_sizzle LOLLL \"oh you're engaged? Congrats!\" No, it's a promise ring....</td>\n",
              "      <td>🙃</td>\n",
              "      <td>👊</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>772980</th>\n",
              "      <td>@josephcaptures hey, do you need some ride money? I can send you some</td>\n",
              "      <td>💓</td>\n",
              "      <td>😂</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601197</th>\n",
              "      <td>@IISuperwomanII Happy Birthday Girl!</td>\n",
              "      <td>🎉</td>\n",
              "      <td>🎉</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263646</th>\n",
              "      <td>@Exoeshowtime: When you dare to ignore Min Yoongi's call to dance to your own ringtone #yoonmin</td>\n",
              "      <td>😂</td>\n",
              "      <td>😂</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453388</th>\n",
              "      <td>@_theboulron: yooooooo i'm fucking crying</td>\n",
              "      <td>😂</td>\n",
              "      <td>😂</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                   text  ... pred\n",
              "229070                       @v_sizzle LOLLL \"oh you're engaged? Congrats!\" No, it's a promise ring....  ...    👊\n",
              "772980                            @josephcaptures hey, do you need some ride money? I can send you some  ...    😂\n",
              "601197                                                             @IISuperwomanII Happy Birthday Girl!  ...    🎉\n",
              "263646  @Exoeshowtime: When you dare to ignore Min Yoongi's call to dance to your own ringtone #yoonmin  ...    😂\n",
              "453388                                                        @_theboulron: yooooooo i'm fucking crying  ...    😂\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv77sdpIrQTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\n",
        "from keras.models import Model\n",
        "from keras.layers import Concatenate\n",
        "\n",
        "def create_char_cnn_model2(num_chars, max_sequence_len, num_labels, drop_out=0.25):\n",
        "    char_input = Input(shape=(max_sequence_len, num_chars), name='char_cnn_input')\n",
        "    \n",
        "    layers = []\n",
        "    for window in (4, 5, 6):\n",
        "        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(char_input)\n",
        "        max_pool_1x = MaxPooling1D(4)(conv_1x)\n",
        "        dropout_1x = Dropout(drop_out)(max_pool_1x)\n",
        "        conv_2x = Conv1D(256, window, activation='relu', padding='valid')(dropout_1x)\n",
        "        max_pool_2x = MaxPooling1D(4)(conv_2x)\n",
        "        dropout_2x = Dropout(drop_out)(max_pool_2x)\n",
        "        layers.append(dropout_2x)\n",
        "        \n",
        "    merged = Concatenate(axis=1)(layers)\n",
        "\n",
        "    dropout = Dropout(drop_out)(merged)\n",
        "    flatten = Flatten()(dropout)\n",
        "    dense = Dense(128, activation='relu')(flatten)\n",
        "    preds = Dense(num_labels, activation='softmax', name='char_cnn_predictions')(dense)\n",
        "    \n",
        "    model = Model(char_input, preds)\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='rmsprop',\n",
        "                  metrics=['acc'])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHsfq1nJ2m55",
        "colab_type": "code",
        "outputId": "ddf2c72b-4ec8-4914-9b18-89a302ab8643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "char_cnn_model2 = create_char_cnn_model2(len(char_to_idx), max_sequence_len, len(emojis))\n",
        "char_cnn_model2.summary()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "char_cnn_input (InputLayer)     (None, 139, 96)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, 136, 128)     49280       char_cnn_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_16 (Conv1D)              (None, 135, 128)     61568       char_cnn_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 134, 128)     73856       char_cnn_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_12 (MaxPooling1D) (None, 34, 128)      0           conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_14 (MaxPooling1D) (None, 33, 128)      0           conv1d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_16 (MaxPooling1D) (None, 33, 128)      0           conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 34, 128)      0           max_pooling1d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 33, 128)      0           max_pooling1d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 33, 128)      0           max_pooling1d_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_15 (Conv1D)              (None, 31, 256)      131328      dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_17 (Conv1D)              (None, 29, 256)      164096      dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 28, 256)      196864      dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_13 (MaxPooling1D) (None, 7, 256)       0           conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_15 (MaxPooling1D) (None, 7, 256)       0           conv1d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_17 (MaxPooling1D) (None, 7, 256)       0           conv1d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 7, 256)       0           max_pooling1d_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 7, 256)       0           max_pooling1d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 7, 256)       0           max_pooling1d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 21, 256)      0           dropout_9[0][0]                  \n",
            "                                                                 dropout_11[0][0]                 \n",
            "                                                                 dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 21, 256)      0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 5376)         0           dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 128)          688256      flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "char_cnn_predictions (Dense)    (None, 121)          15609       dense_13[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,380,857\n",
            "Trainable params: 1,380,857\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpPP8Gme3LOJ",
        "colab_type": "code",
        "outputId": "bbc7e47c-94ff-4609-dd74-1c0cfb020a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        }
      },
      "source": [
        "early = keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                              min_delta=0.03,\n",
        "                              patience=2,\n",
        "                              verbose=0, mode='auto')\n",
        "\n",
        "BATCH_SIZE = 2048\n",
        "char_cnn_model2.fit_generator(\n",
        "    data_generator(train_tweets, batch_size=BATCH_SIZE),\n",
        "    epochs=30,\n",
        "    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n",
        "    verbose=2,\n",
        "    callbacks=[early]\n",
        ")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            " - 135s - loss: 3.8482 - acc: 0.1862\n",
            "Epoch 2/30\n",
            " - 133s - loss: 3.4903 - acc: 0.2330\n",
            "Epoch 3/30\n",
            " - 133s - loss: 3.3271 - acc: 0.2632\n",
            "Epoch 4/30\n",
            " - 133s - loss: 3.2139 - acc: 0.2842\n",
            "Epoch 5/30\n",
            " - 133s - loss: 3.1342 - acc: 0.2995\n",
            "Epoch 6/30\n",
            " - 133s - loss: 3.0657 - acc: 0.3119\n",
            "Epoch 7/30\n",
            " - 133s - loss: 3.0128 - acc: 0.3224\n",
            "Epoch 8/30\n",
            " - 133s - loss: 2.9683 - acc: 0.3304\n",
            "Epoch 9/30\n",
            " - 132s - loss: 2.9299 - acc: 0.3379\n",
            "Epoch 10/30\n",
            " - 133s - loss: 2.8970 - acc: 0.3438\n",
            "Epoch 11/30\n",
            " - 133s - loss: 2.8659 - acc: 0.3496\n",
            "Epoch 12/30\n",
            " - 133s - loss: 2.8415 - acc: 0.3542\n",
            "Epoch 13/30\n",
            " - 133s - loss: 2.8050 - acc: 0.3617\n",
            "Epoch 14/30\n",
            " - 132s - loss: 2.7888 - acc: 0.3644\n",
            "Epoch 15/30\n",
            " - 132s - loss: 2.7661 - acc: 0.3687\n",
            "Epoch 16/30\n",
            " - 132s - loss: 2.7506 - acc: 0.3714\n",
            "Epoch 17/30\n",
            " - 132s - loss: 2.7353 - acc: 0.3743\n",
            "Epoch 18/30\n",
            " - 132s - loss: 2.7190 - acc: 0.3771\n",
            "Epoch 19/30\n",
            " - 131s - loss: 2.7066 - acc: 0.3791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f065d374518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qj8piud4UlX",
        "colab_type": "code",
        "outputId": "920a532a-8acd-43b0-b5c0-f329908e2cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "char_cnn_model2.evaluate_generator(\n",
        "    data_generator(test_tweets, batch_size=BATCH_SIZE),\n",
        "    steps=len(test_tweets) / BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.8042608329227994, 0.37989676339285716]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXTViWJMetY3",
        "colab_type": "text"
      },
      "source": [
        "**Featurizing and preparing our data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mRnUFWue0rg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 50000\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(tweets[\"text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofrBk6_hfty0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_tokens = tokenizer.texts_to_sequences(train_tweets[\"text\"])\n",
        "test_tokens = tokenizer.texts_to_sequences(test_tweets[\"text\"])\n",
        "max_num_tokens = max(len(x) for x in chain(training_tokens, test_tokens))\n",
        "training_tokens = pad_sequences(training_tokens, maxlen=max_num_tokens)\n",
        "test_tokens = pad_sequences(test_tokens, maxlen=max_num_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQdwqkGRiH70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_labels = np.asarray([emoji_to_idx[em] for em in train_tweets[\"emoji\"]])\n",
        "test_labels = np.asarray([emoji_to_idx[em] for em in test_tweets['emoji']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyFF2pmYjsaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def load_weights(tokenizer):\n",
        "#     w2v_model = Word2Vec.load(\"data/twitter_w2v.model\") # model 没有给出\n",
        "#     w2v = np.zeros((tokenizer.num_wrods, w2v_model.syn0.shape[1]))\n",
        "#     for k, v in tokenizer.word_index.items():\n",
        "#         if v >= tokenzier.num_words:\n",
        "#             continue\n",
        "#         if k in w2v_model:\n",
        "#             w2v[v] = w2v_model[k]\n",
        "#     return w2v\n",
        "# w2v = load_weights(tokenizer)\n",
        "# w2v.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByG82_6b-eyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import gensim\n",
        "\n",
        "CACHE_DIR = os.path.expanduser('~/.cache/dl-cookbook')\n",
        "\n",
        "def download(url):\n",
        "    filename = os.path.join(CACHE_DIR, re.sub('[^a-zA-Z0-9.]+', '_', url))\n",
        "    if os.path.exists(filename):\n",
        "        return filename\n",
        "    else:\n",
        "        os.system('mkdir -p \"%s\"' % CACHE_DIR)\n",
        "        assert os.system('wget -O \"%s\" \"%s\"' % (filename, url)) == 0\n",
        "        return filename\n",
        "    \n",
        "    \n",
        "def load_w2v(tokenizer=None):\n",
        "    word2vec_gz = download('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')\n",
        "    word2vec_vectors = word2vec_gz.replace('.gz', '')\n",
        "    if not os.path.exists(word2vec_vectors):\n",
        "        assert os.system('gunzip -d --keep \"%s\"' % word2vec_gz) == 0\n",
        "        \n",
        "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_vectors, binary=True)\n",
        "    \n",
        "#     total_count = sum(tokenizer.word_counts.values())\n",
        "#     idf_dict = { k: np.log(total_count/v) for (k,v) in tokenizer.word_counts.items() }\n",
        "    \n",
        "    w2v = np.zeros((tokenizer.num_words, w2v_model.syn0.shape[1]))\n",
        "#     idf = np.zeros((tokenizer.num_words, 1))\n",
        "\n",
        "    for k, v in tokenizer.word_index.items():\n",
        "        if v >= tokenizer.num_words:\n",
        "            continue\n",
        "\n",
        "        if k in w2v_model:\n",
        "            w2v[v] = w2v_model[k]\n",
        "#             idf[v] = idf_dict[k]\n",
        "\n",
        "    del w2v_model\n",
        "    return w2v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cbaUOmM_Adk",
        "colab_type": "code",
        "outputId": "e2fc673d-bcb9-44b2-ffc7-a37d8edfadcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "# This may take a while to load\n",
        "w2v = load_w2v(tokenizer)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8MNN3M7_-nt",
        "colab_type": "code",
        "outputId": "15b659f7-7f29-42e7-fd95-2b219a17a3be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "w2v.shape"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR8a-OiNCFY-",
        "colab_type": "text"
      },
      "source": [
        "**World Level CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvmrgQFKAKvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers, models\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "def make_embedding(name, vocab_size, embedding_size, weights=None, mask_zero=True):\n",
        "    if weights is not None:\n",
        "        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, \n",
        "                                output_dim=weights.shape[1], \n",
        "                                weights=[weights], trainable=False, \n",
        "                                name='%s/embedding' % name)\n",
        "    else:\n",
        "        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, \n",
        "                                output_dim=embedding_size,\n",
        "                                name='%s/embedding' % name)\n",
        "    \n",
        "def create_cnn_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
        "    assert not (embedding_size is None and embedding_weights is None)\n",
        "    message = Input(shape=(max_num_tokens, ), dtype='int32', name='cnn_input')\n",
        "    # The convolution layer in keras does not support masking, so we just allow\n",
        "    # the embedding layer to learn an explicit value.\n",
        "    embedding = make_embedding(\"cnn_embedding\", \n",
        "                               vocab_size, embedding_size, \n",
        "                               embedding_weights, mask_zero=False)(message)\n",
        "    global_pools = []\n",
        "    for window in 2, 3:\n",
        "        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(embedding)\n",
        "        max_pool_1x = MaxPooling1D(2)(conv_1x)\n",
        "        conv_2x = Conv1D(256, window, activation='relu', padding='valid')(max_pool_1x)\n",
        "        max_pool_2x = MaxPooling1D(2)(conv_2x)\n",
        "        conv_3x = Conv1D(256, window, activation='relu', padding='valid')(max_pool_2x)\n",
        "        global_pools.append(GlobalMaxPooling1D()(conv_3x))\n",
        "        \n",
        "    merged = Concatenate(axis=1)(global_pools)\n",
        "    fc1 = Dense(units=128, activation='elu')(merged)\n",
        "    preds = Dense(units=len(emojis), activation='softmax', name='cnn_predictions')(fc1)\n",
        "    model = Model(\n",
        "        inputs=[message],\n",
        "        outputs=[preds],\n",
        "    )\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T91zcxuHu4m",
        "colab_type": "code",
        "outputId": "fb8a3444-4a5f-4d53-80c0-5e10b02d6825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "source": [
        "cnn_model = create_cnn_model(VOCAB_SIZE, embedding_weights=w2v)\n",
        "cnn_model.summary()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cnn_input (InputLayer)          (None, 54)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "cnn_embedding/embedding (Embedd (None, 54, 300)      15000000    cnn_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, 53, 128)      76928       cnn_embedding/embedding[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 52, 128)      115328      cnn_embedding/embedding[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_18 (MaxPooling1D) (None, 26, 128)      0           conv1d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_20 (MaxPooling1D) (None, 26, 128)      0           conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_21 (Conv1D)              (None, 25, 256)      65792       max_pooling1d_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_24 (Conv1D)              (None, 24, 256)      98560       max_pooling1d_20[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_19 (MaxPooling1D) (None, 12, 256)      0           conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_21 (MaxPooling1D) (None, 12, 256)      0           conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_22 (Conv1D)              (None, 11, 256)      131328      max_pooling1d_19[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_25 (Conv1D)              (None, 10, 256)      196864      max_pooling1d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 256)          0           conv1d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 256)          0           conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 512)          0           global_max_pooling1d_2[0][0]     \n",
            "                                                                 global_max_pooling1d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 128)          65664       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "cnn_predictions (Dense)         (None, 121)          15609       dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 15,766,073\n",
            "Trainable params: 766,073\n",
            "Non-trainable params: 15,000,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NO88NDGIBr4",
        "colab_type": "code",
        "outputId": "3ac4d052-9b72-428b-ffec-f503aab63b68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "cnn_model.fit(training_tokens, training_labels, epochs=5)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "634695/634695 [==============================] - 115s 182us/step - loss: 3.3670 - acc: 0.2700\n",
            "Epoch 2/5\n",
            "634695/634695 [==============================] - 113s 178us/step - loss: 3.2478 - acc: 0.2930\n",
            "Epoch 3/5\n",
            "634695/634695 [==============================] - 114s 179us/step - loss: 3.2565 - acc: 0.2963\n",
            "Epoch 4/5\n",
            "634695/634695 [==============================] - 113s 177us/step - loss: 3.2754 - acc: 0.2953\n",
            "Epoch 5/5\n",
            "634695/634695 [==============================] - 112s 177us/step - loss: 3.2834 - acc: 0.2948\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0652ead2e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrjhOdGKIN8a",
        "colab_type": "text"
      },
      "source": [
        "Le**arning Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdM89gKaIXHx",
        "colab_type": "code",
        "outputId": "ea47eabb-1fba-4e34-9638-bd2a56dd6cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "source": [
        "learned_embeddings_cnn_model = create_cnn_model(VOCAB_SIZE, embedding_size=100)\n",
        "learned_embeddings_cnn_model.summary()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cnn_input (InputLayer)          (None, 54)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "cnn_embedding/embedding (Embedd (None, 54, 100)      5000000     cnn_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_26 (Conv1D)              (None, 53, 128)      25728       cnn_embedding/embedding[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_29 (Conv1D)              (None, 52, 128)      38528       cnn_embedding/embedding[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_22 (MaxPooling1D) (None, 26, 128)      0           conv1d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_24 (MaxPooling1D) (None, 26, 128)      0           conv1d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_27 (Conv1D)              (None, 25, 256)      65792       max_pooling1d_22[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_30 (Conv1D)              (None, 24, 256)      98560       max_pooling1d_24[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_23 (MaxPooling1D) (None, 12, 256)      0           conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_25 (MaxPooling1D) (None, 12, 256)      0           conv1d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_28 (Conv1D)              (None, 11, 256)      131328      max_pooling1d_23[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_31 (Conv1D)              (None, 10, 256)      196864      max_pooling1d_25[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_4 (GlobalM (None, 256)          0           conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_5 (GlobalM (None, 256)          0           conv1d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 512)          0           global_max_pooling1d_4[0][0]     \n",
            "                                                                 global_max_pooling1d_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 128)          65664       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "cnn_predictions (Dense)         (None, 121)          15609       dense_15[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 5,638,073\n",
            "Trainable params: 5,638,073\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lP5jTwkI2Ku",
        "colab_type": "code",
        "outputId": "e584bb55-d4ee-4185-deb2-2af294e13022",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "learned_embeddings_cnn_model.fit(training_tokens, training_labels, epochs=5)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "634695/634695 [==============================] - 168s 264us/step - loss: 3.3041 - acc: 0.2798\n",
            "Epoch 2/5\n",
            "634695/634695 [==============================] - 166s 262us/step - loss: 3.0324 - acc: 0.3336\n",
            "Epoch 3/5\n",
            "634695/634695 [==============================] - 166s 262us/step - loss: 2.9784 - acc: 0.3485\n",
            "Epoch 4/5\n",
            "634695/634695 [==============================] - 167s 264us/step - loss: 2.9797 - acc: 0.3523\n",
            "Epoch 5/5\n",
            "634695/634695 [==============================] - 167s 264us/step - loss: 2.9834 - acc: 0.3540\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0652ead470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhc8eRApZwW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learned_embeddings_cnn_model.save('zoo/07/twitter_learned_embeddings_cnn_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaJcsm4abhOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Masking\n",
        "\n",
        "def create_lstm_model(vocab_size,  embedding_size=None, embedding_weights=None):\n",
        "    assert not (embedding_size is None and embedding_weights is None)\n",
        "    message = Input(shape=(max_num_tokens, ), dtype='int32', name='lstm_input')\n",
        "    embedding = Embedding(mask_zero=True, input_dim=vocab_size, \n",
        "                          output_dim=embedding_weights.shape[1], \n",
        "                          weights=[embedding_weights],\n",
        "                          trainable=True,\n",
        "                          name='lstm_embedding')(message)\n",
        "    mask = Masking(mask_value=0)(embedding)\n",
        "    lstm_1 = LSTM(units=128, return_sequences=False)(mask)\n",
        "    preds = Dense(units=len(emojis), activation='softmax', name='lstm_predictions')(lstm_1)\n",
        "    model = Model(\n",
        "        inputs=[message],\n",
        "        outputs=[preds],\n",
        "    )\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpTsXinJh3Yp",
        "colab_type": "code",
        "outputId": "a8d48547-c1f5-4b01-cf36-182851553f8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "lstm_model = create_lstm_model(VOCAB_SIZE, embedding_weights=w2v)\n",
        "lstm_model.summary()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_input (InputLayer)      (None, 54)                0         \n",
            "_________________________________________________________________\n",
            "lstm_embedding (Embedding)   (None, 54, 300)           15000000  \n",
            "_________________________________________________________________\n",
            "masking_3 (Masking)          (None, 54, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               219648    \n",
            "_________________________________________________________________\n",
            "lstm_predictions (Dense)     (None, 121)               15609     \n",
            "=================================================================\n",
            "Total params: 15,235,257\n",
            "Trainable params: 15,235,257\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TljYu8CIjcxC",
        "colab_type": "code",
        "outputId": "23a123a2-9971-4f01-8072-c13a9f3bf996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "early = keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                              min_delta=0.03,\n",
        "                              patience=2,\n",
        "                              verbose=0, mode='auto')\n",
        "\n",
        "lstm_model.fit(training_tokens, training_labels, epochs=12, batch_size=1024, callbacks=[early])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "634695/634695 [==============================] - 80s 126us/step - loss: 3.4324 - acc: 0.2523\n",
            "Epoch 2/12\n",
            "634695/634695 [==============================] - 79s 125us/step - loss: 3.0524 - acc: 0.3139\n",
            "Epoch 3/12\n",
            "634695/634695 [==============================] - 79s 124us/step - loss: 2.8974 - acc: 0.3427\n",
            "Epoch 4/12\n",
            "634695/634695 [==============================] - 79s 125us/step - loss: 2.7839 - acc: 0.3637\n",
            "Epoch 5/12\n",
            "634695/634695 [==============================] - 79s 124us/step - loss: 2.6893 - acc: 0.3822\n",
            "Epoch 6/12\n",
            "634695/634695 [==============================] - 79s 125us/step - loss: 2.6051 - acc: 0.3980\n",
            "Epoch 7/12\n",
            "634695/634695 [==============================] - 79s 124us/step - loss: 2.5275 - acc: 0.4128\n",
            "Epoch 8/12\n",
            "634695/634695 [==============================] - 79s 124us/step - loss: 2.4546 - acc: 0.4268\n",
            "Epoch 9/12\n",
            "634695/634695 [==============================] - 79s 125us/step - loss: 2.3851 - acc: 0.4407\n",
            "Epoch 10/12\n",
            "634695/634695 [==============================] - 79s 125us/step - loss: 2.3187 - acc: 0.4544\n",
            "Epoch 11/12\n",
            "634695/634695 [==============================] - 79s 125us/step - loss: 2.2542 - acc: 0.4675\n",
            "Epoch 12/12\n",
            "634695/634695 [==============================] - 79s 125us/step - loss: 2.1927 - acc: 0.4800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0656572dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oag46x77fZxL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9a7b8802-3db8-4347-ffdd-79aa0b61f1e7"
      },
      "source": [
        "lstm_model.evaluate(test_tokens, test_labels)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70522/70522 [==============================] - 80s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.9637703841747007, 0.3706077536104933]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7wksfxfp9f",
        "colab_type": "text"
      },
      "source": [
        "**Comparing our models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT3_KT91f7TK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "43a59bc0-8c12-42fa-9d96-a6dc61dd3f27"
      },
      "source": [
        "test_char_vectors, _ = next(data_generator(test_tweets, None))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-910a6ae52f6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_char_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data_generator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuwmMpsEf_DB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = {\n",
        "    label: [emojis[np.argmax(x)] for x in pred]\n",
        "    for label, pred in (\n",
        "        ('lstm', lstm_model.predict(test_tokens[:100])),\n",
        "        ('char_cnn', char_cnn_model.predict(test_char_vectors[:100])),\n",
        "        ('cnn', cnn_model.predict(test_tokens[:100])),\n",
        "    )\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag3k2n-fm0LX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a dataframe just for test data\n",
        "pd.options.display.max_colwidth = 128\n",
        "test_df = test_tweets[:100].reset_index()\n",
        "eval_df = pd.DataFrame({\n",
        "    'content': test_df['text'],\n",
        "    'true': test_df['emoji'],\n",
        "    **predictions\n",
        "})\n",
        "eval_df[['content', 'true', 'char_cnn', 'cnn', 'lstm']].head(25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSdtHdHIoBe9",
        "colab_type": "text"
      },
      "source": [
        "**Qualitative Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-0vZAhXoa32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_df[eval_df['lstm'] != eval_df['true']].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s41MoGNIoeWZ",
        "colab_type": "text"
      },
      "source": [
        "**Ensemble model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d--WalZokZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combined_data_generator(tweets, tokens, batch_size):\n",
        "    tweets = tweets.reset_index()\n",
        "    while True:\n",
        "        batch_idx = random.sample(range(len(tweets)), batch_size)\n",
        "        tweet_batch = tweets.iloc[batch_idx]\n",
        "        token_batch = tokens[batch_idx]\n",
        "        char_vec = np.zeros((batch_size, max_sequence_len, len(chars)))\n",
        "        token_vec = np.zeros((batch_size, max_num_tokens))\n",
        "        y = np.zeros((batch_size,))\n",
        "        for row_idx, (token_row, (_, tweet_row)) in enumerate(zip(token_batch, tweet_batch.iterrows())):\n",
        "            y[row_idx] = emoji_to_idx[tweet_row['emoji']]\n",
        "            for ch_idx, ch in enumerate(tweet_row['text']):\n",
        "                char_vec[row_idx, ch_idx, char_to_idx[ch]] = 1\n",
        "            token_vec[row_idx, :] = token_row\n",
        "        yield {'char_cnn_input': char_vec, 'cnn_input': token_vec, 'lstm_input': token_vec}, y\n",
        "\n",
        "d, y = next(combined_data_generator(train_tweets, training_tokens, 5))\n",
        "d['lstm_input'].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg-5HEyIooQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction_layer(model):\n",
        "    layers = [layer for layer in model.layers if layer.name.endswith('_predictions')]\n",
        "    return layers[0].output\n",
        "\n",
        "def create_ensemble(*models):\n",
        "    inputs = [model.input for model in models]\n",
        "    predictions = [prediction_layer(model) for model in models]\n",
        "    merged = Average()(predictions)\n",
        "    model = Model(\n",
        "        inputs=inputs,\n",
        "        outputs=[merged],\n",
        "    )\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "ensemble = create_ensemble(char_cnn_model2, cnn_model, lstm_model)\n",
        "ensemble.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlDON182oweV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "ensemble.fit_generator(\n",
        "    combined_data_generator(train_tweets, training_tokens, BATCH_SIZE),\n",
        "    epochs=20,\n",
        "    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n",
        "    verbose=2,\n",
        "    callbacks=[early]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQdTHoIgo3Lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ensemble.evaluate_generator(\n",
        "    combined_data_generator(test_tweets, test_tokens, BATCH_SIZE),\n",
        "    steps=len(test_tweets) / BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUa4iDe7pkAz",
        "colab_type": "text"
      },
      "source": [
        "**Tweet Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrzcr0tKqX0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "53bddfbf-0f77-4bbc-aad2-a4d87c3fe4eb"
      },
      "source": [
        "import random\n",
        "import twitter\n",
        "import emoji\n",
        "import gensim\n",
        "import unicodedata\n",
        "import html\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvmLblyaqbP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fill these in!\n",
        "\n",
        "CONSUMER_KEY = 'xbMuxcJpRTiVGt2C2EYnA'\n",
        "CONSUMER_SECRET = '2DbQTsvIptkPTdaUcos8DDvQH9fzO0hNjJpUT2uVzQ'\n",
        "ACCESS_TOKEN = '7319442-EDm4CPxL7W4KkZcGWRMJNVHp88W5OH9vgblu898fg'\n",
        "ACCESS_SECRET = '5ZxJSbqXhG7uhgXzTFWf9XhkfsxxinlPRXyDTzbA9w'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3BXEfRwqeKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aca4db3c-2e07-46ef-d0f6-09bd3dbd00f2"
      },
      "source": [
        "auth=twitter.OAuth(\n",
        "    consumer_key=CONSUMER_KEY,\n",
        "    consumer_secret=CONSUMER_SECRET,\n",
        "    token=ACCESS_TOKEN,\n",
        "    token_secret=ACCESS_SECRET,\n",
        ")\n",
        "\n",
        "status_stream = twitter.TwitterStream(auth=auth).statuses\n",
        "next(status_stream.sample()).keys()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['delete'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K177u4hh9aJX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "da4ea18a-b644-49b1-9a2e-f0555253b6ea"
      },
      "source": [
        "RE_URL = re.compile(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?')\n",
        "RE_WHITESPACE = re.compile(r'\\s+')\n",
        "\n",
        "text = \"RT @SLOWTOWNVHS: tried to unfollow the most annoying bitch on twitter but all it said was edit profile \\n\\n https://t.co/FZ2YWcbUwM\"\n",
        "text = html.unescape(text)\n",
        "print(text)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RT @SLOWTOWNVHS: tried to unfollow the most annoying bitch on twitter but all it said was edit profile \n",
            "\n",
            " https://t.co/FZ2YWcbUwM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGVdeyjj_f9a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c409645-481a-486d-effe-efe91faed190"
      },
      "source": [
        "text = RE_WHITESPACE.sub(' ', text)\n",
        "print(text)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RT @SLOWTOWNVHS: tried to unfollow the most annoying bitch on twitter but all it said was edit profile https://t.co/FZ2YWcbUwM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXH2NUn5Aad0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e1658729-d0c6-4291-a91f-1b48bf23eeb7"
      },
      "source": [
        "text = RE_URL.sub(' ', text)\n",
        "print(text)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RT @SLOWTOWNVHS: tried to unfollow the most annoying bitch on twitter but all it said was edit profile  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhbV9okaAkqS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cd41a696-bfc3-4348-9f4e-7de98bba825a"
      },
      "source": [
        "text = strip_accents(text)\n",
        "print(text)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RT @SLOWTOWNVHS: tried to unfollow the most annoying bitch on twitter but all it said was edit profile  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsMl9D4BA4oR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "85a5559e-49ae-47d5-f328-9bc03921980f"
      },
      "source": [
        "text = ''.join(ch for ch in text if ord(ch) < 128)\n",
        "print(text)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RT @SLOWTOWNVHS: tried to unfollow the most annoying bitch on twitter but all it said was edit profile  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnRTRQbkCTma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1264e189-c14c-45a0-b5fd-0845dc4e6df7"
      },
      "source": [
        "text = text[3:].strip()\n",
        "text"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'@SLOWTOWNVHS: tried to unfollow the most annoying bitch on twitter but all it said was edit profile'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxASHDmMCl2x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "669ead23-8e05-4572-eec5-c2cbaf383114"
      },
      "source": [
        "text = text_to_word_sequence(text)\n",
        "text"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['slowtownvhs',\n",
              " 'tried',\n",
              " 'to',\n",
              " 'unfollow',\n",
              " 'the',\n",
              " 'most',\n",
              " 'annoying',\n",
              " 'bitch',\n",
              " 'on',\n",
              " 'twitter',\n",
              " 'but',\n",
              " 'all',\n",
              " 'it',\n",
              " 'said',\n",
              " 'was',\n",
              " 'edit',\n",
              " 'profile']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvK6lRbJqga-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "a7ff338c-3259-49a9-a38c-feb9979a7537"
      },
      "source": [
        "RE_URL = re.compile(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?')\n",
        "RE_WHITESPACE = re.compile(r'\\s+')\n",
        "\n",
        "def strip_accents(s):\n",
        "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s)\n",
        "                   if unicodedata.category(c) != \"Mn\")\n",
        "\n",
        "class TokensYielder(object):\n",
        "    def __init__(self, tweet_count, stream):\n",
        "        self.tweet_count = tweet_count\n",
        "        self.stream = stream\n",
        "        \n",
        "    def __iter__(self):\n",
        "        count = self.tweet_count\n",
        "        for tweet in self.stream:\n",
        "            if tweet.get(\"lang\") != \"en\":\n",
        "                continue\n",
        "            text = text[\"text\"]\n",
        "            text = html.unescape(text)\n",
        "            text = RE_WHITESPACE.sub(' ', text)\n",
        "            text = RE_URL.sub(' ', text)\n",
        "            text = strip_accents(text)\n",
        "            text = ''.join(ch for ch in text if ord(ch) < 128)\n",
        "            if text.startwith(\"RT \"):\n",
        "                text = text[3:]\n",
        "                \n",
        "            text = text.strip()\n",
        "            if text:\n",
        "                yield text_to_word_sequence(text)\n",
        "                count -= 1\n",
        "                if count <= 0:\n",
        "                    break\n",
        "for t in TokensYielder(3, twitter.TwitterStream(auth=auth).statuses.sample()):\n",
        "    print(t)                    "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b48a7e429c05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTokensYielder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTwitterStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatuses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b48a7e429c05>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lang\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munescape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRE_WHITESPACE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'text' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEOvbVae1IMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = list(TokensYielder(70000, twitter.TwitterStream(auth=auth).statuses.sample()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoKN5Dqx1JE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.Word2Vec(tweets, \n",
        "                               workers=5,\n",
        "                               min_count=2,\n",
        "                              )\n",
        "model.save('zoo/07/twitter_stream_w2v.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U15Rm3YS1PGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.wv.most_similar(positive=['love'], topn=5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
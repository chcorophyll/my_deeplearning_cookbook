{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my_Question matching.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chcorophyll/my_deeplearning_cookbook/blob/master/my_Question_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzw_MkoORmh1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "51ccf34c-c363-401f-c0bb-01e19818bacd"
      },
      "source": [
        "!git clone https://github.com/chcorophyll/deep_learning_cookbook.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep_learning_cookbook'...\n",
            "remote: Enumerating objects: 427, done.\u001b[K\n",
            "remote: Total 427 (delta 0), reused 0 (delta 0), pack-reused 427\u001b[K\n",
            "Receiving objects: 100% (427/427), 160.26 MiB | 26.49 MiB/s, done.\n",
            "Resolving deltas: 100% (207/207), done.\n",
            "Checking out files: 100% (86/86), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAoN-rXjSPqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eba5ec54-5e86-410c-f821-3283b2ef3501"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deep_learning_cookbook\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kXnOyoVRslK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "path_org = os.getcwd()\n",
        "data_path = os.path.join(path_org, \"deep_learning_cookbook\")\n",
        "os.chdir(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys8XiWgPIZod",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "cellView": "both",
        "outputId": "4d59cefc-89b7-46ef-ebc4-505b8436a24c"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras import layers, models, utils\n",
        "import json"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTwg22tBJ0ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reset_everything():\n",
        "    import tensorflow as tf\n",
        "    %reset -f in out dhist\n",
        "    tf.reset_default_graph()\n",
        "    K.set_session(tf.InteractiveSession())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvOUmasjKT92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants for our networks.  We keep these deliberately small to reduce training time.\n",
        "VOCAB_SIZE = 250000\n",
        "EMBEDDING_SIZE = 100\n",
        "MAX_DOC_LEN = 128\n",
        "MIN_DOC_LEN = 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTm1pr-EKrK2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "d2c63281-3494-47d5-f623-3df0a6c13f40"
      },
      "source": [
        "def extract_stackexchange(filename, limit=1000000):\n",
        "    json_file = filename + 'limit=%s.json' % limit\n",
        "\n",
        "    rows = []\n",
        "    for i, line in enumerate(os.popen('7z x -so \"%s\" Posts.xml' % filename)):\n",
        "        line = str(line)\n",
        "        if not line.startswith('  <row'):\n",
        "            continue\n",
        "            \n",
        "        if i % 1000 == 0:\n",
        "            print('\\r%05d/%05d' % (i, limit), end='', flush=True)\n",
        "\n",
        "        parts = line[6:-5].split('\"')\n",
        "        record = {}\n",
        "        for i in range(0, len(parts), 2):\n",
        "            k = parts[i].replace('=', '').strip()\n",
        "            v = parts[i+1].strip()\n",
        "            record[k] = v\n",
        "        rows.append(record)\n",
        "        \n",
        "        if len(rows) > limit:\n",
        "            break\n",
        "    \n",
        "    with open(json_file, 'w') as fout:\n",
        "        json.dump(rows, fout)\n",
        "    \n",
        "    return rows\n",
        "\n",
        "\n",
        "xml_7z = utils.get_file(\n",
        "    fname='travel.stackexchange.com.7z',\n",
        "    origin='https://ia800107.us.archive.org/27/items/stackexchange/travel.stackexchange.com.7z',\n",
        ")\n",
        "print()\n",
        "\n",
        "rows = extract_stackexchange(xml_7z)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://ia800107.us.archive.org/27/items/stackexchange/travel.stackexchange.com.7z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1318\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m         self.sock = self._create_connection(\n\u001b[0;32m--> 936\u001b[0;31m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 544\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1361\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 110] Connection timed out>",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7cfb2dc7e981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m xml_7z = utils.get_file(\n\u001b[1;32m     31\u001b[0m     \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'travel.stackexchange.com.7z'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://ia800107.us.archive.org/27/items/stackexchange/travel.stackexchange.com.7z'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: URL fetch failure on https://ia800107.us.archive.org/27/items/stackexchange/travel.stackexchange.com.7z: None -- [Errno 110] Connection timed out"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EocApfkgSwvK",
        "colab_type": "text"
      },
      "source": [
        "**Data Exploration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHbasHRsQ029",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame.from_records(rows)\n",
        "df = df.set_index(\"Id\", drop=False)\n",
        "df[\"Title\"] = df[\"Title\"].fillna(\"\").astype(\"str\")\n",
        "df['Tags'] = df['Tags'].fillna('').astype('str')\n",
        "df['Body'] = df['Body'].fillna('').astype('str')\n",
        "df['Id'] = df['Id'].astype('int')\n",
        "df['PostTypeId'] = df['PostTypeId'].astype('int')\n",
        "df['ViewCount'] = df['ViewCount'].astype('float')\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6dJfwvSThwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(df[df['ViewCount'] > 250000]['Title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27n-DUTTTz-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(df[\"Body\"] + df[\"Title\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt_GH7zfWIqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute TF/IDF Values\n",
        "\n",
        "total_count = sum(tokenizer.word_counts.values())\n",
        "idf = { k: np.log(total_count/v) for (k,v) in tokenizer.word_counts.items() }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPlj57nRXbow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download pre-trained word2vec embeddings\n",
        "\n",
        "import gensim\n",
        "\n",
        "glove_100d = utils.get_file(\n",
        "    fname='glove.6B.100d.txt',\n",
        "    origin='https://storage.googleapis.com/deep-learning-cookbook/glove.6B.100d.txt',\n",
        ")\n",
        "\n",
        "w2v_100d = glove_100d + \".w2v\"\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "glove2word2vec(glove_100d, w2v_100d)\n",
        "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_100d)\n",
        "w2v_weights = np.zeros((VOCAB_SIZE, w2v_model.syn0.shape[1]))\n",
        "idf_weights = np.zeros((VOCAB_SIZE, 1))\n",
        "\n",
        "for k, v in tokenizer.word_index.items():\n",
        "    if v >= VOCAB_SIZE:\n",
        "        continue\n",
        "    \n",
        "    if k in w2v_model:\n",
        "        w2v_weights[v] = w2v_model[k]\n",
        "        \n",
        "    idf_weights[v] = idf[k]\n",
        "    \n",
        "del w2v_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUkMhDjd5zI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['title_tokens'] = tokenizer.texts_to_sequences(df['Title'])\n",
        "df['body_tokens'] = tokenizer.texts_to_sequences(df['Body'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri8Idd59evlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "# We can create a data generator that will randomly title and body tokens for questions.  We'll use random text\n",
        "# from other questions as a negative example when necessary.\n",
        "\n",
        "def data_generator(batch_size, negative_samples=1):\n",
        "    questions = df[df['PostTypeId'] == 1]\n",
        "    all_q_ids = list(questions.index)\n",
        "    \n",
        "    batch_x_a = []\n",
        "    batch_x_b = []\n",
        "    batch_y = []\n",
        "    \n",
        "    def _add(x_a, x_b, y):\n",
        "        bacth_x_a.append(x_a[:MAX_DOC_LEN])\n",
        "        batch_x_b.append(x_b[:MAX_DOC_LEN])\n",
        "        batch_y.append(y)\n",
        "    \n",
        "    while True:\n",
        "        questions = questions.sample(frac=1.0)\n",
        "        \n",
        "        for i, q in questions.iterrows():\n",
        "            _add(q['title_tokens'], q['body_tokens'], 1)\n",
        "            \n",
        "            negative_q = random.sample(all_q_ids, negative_samples)\n",
        "            for nq_id in negative_q:\n",
        "                _add(q['title_tokens'], df.at[nq_id, 'body_tokens'], 0)            \n",
        "            \n",
        "            if len(batch_y) >= batch_size:\n",
        "                yield ({\n",
        "                    'title': pad_sequences(batch_x_a, maxlen=None),\n",
        "                    'body': pad_sequences(batch_x_b, maxlen=None),\n",
        "                }, np.asarray(batch_y))\n",
        "                \n",
        "                batch_x_a = []\n",
        "                batch_x_b = []\n",
        "                batch_y = []\n",
        "\n",
        "# dg = data_generator(1, 2)\n",
        "# next(dg)\n",
        "# next(dg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-yQvAl8hnU0",
        "colab_type": "text"
      },
      "source": [
        "**Embedding Lookups**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yHrCIk4hrBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questions = df[df['PostTypeId'] == 1]['Title'].reset_index(drop=True)\n",
        "question_tokens = pad_sequences(tokenizer.texts_to_sequences(questions))\n",
        "\n",
        "class EmbeddingWrapper(object):\n",
        "    def __init__(self, model):\n",
        "        self._r = questions\n",
        "        self._i = {i:s for (i, s) in enumerate(questions)}\n",
        "        self._w = model.predict({'title': question_tokens}, verbose=1, batch_size=1024)\n",
        "        self._model = model\n",
        "        self._norm = np.sqrt(np.sum(self._w * self._w + 1e-5, axis=1))\n",
        "\n",
        "    def nearest(self, sentence, n=10):\n",
        "        x = tokenizer.texts_to_sequences([sentence])\n",
        "        if len(x[0]) < MIN_DOC_LEN:\n",
        "            x[0] += [0] * (MIN_DOC_LEN - len(x))\n",
        "        e = self._model.predict(np.asarray(x))[0]\n",
        "        norm_e = np.sqrt(np.dot(e, e))\n",
        "        dist = np.dot(self._w, e) / (norm_e * self._norm)\n",
        "\n",
        "        top_idx = np.argsort(dist)[-n:]\n",
        "        return pd.DataFrame.from_records([\n",
        "            {'question': self._r[i], 'dist': float(dist[i])}\n",
        "            for i in top_idx\n",
        "        ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llFlf_OFjOqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our first model will just sum up the embeddings of each token.\n",
        "# The similarity between documents will be the dot product of the final embedding.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def sum_model(embedding_size, vocab_size, embedding_weights=None, idf_weights=None):\n",
        "    title = layers.Input(shape=(None,), dtype='int32', name='title')\n",
        "    body = layers.Input(shape=(None,), dtype='int32', name='body')\n",
        "\n",
        "    def make_embedding(name):\n",
        "        if embedding_weights is not None:\n",
        "            embedding = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=w2v_weights.shape[1], \n",
        "                                         weights=[w2v_weights], trainable=False, \n",
        "                                         name='%s/embedding' % name)\n",
        "        else:\n",
        "            embedding = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=embedding_size,\n",
        "                                        name='%s/embedding' % name)\n",
        "\n",
        "        if idf_weights is not None:\n",
        "            idf = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=1, \n",
        "                                   weights=[idf_weights], trainable=False,\n",
        "                                   name='%s/idf' % name)\n",
        "        else:\n",
        "            idf = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=1,\n",
        "                                   name='%s/idf' % name)\n",
        "            \n",
        "        return embedding, idf\n",
        "    \n",
        "    embedding_a, idf_a = make_embedding('a')\n",
        "    embedding_b, idf_b = embedding_a, idf_a\n",
        "#     embedding_b, idf_b = make_embedding('b')\n",
        "\n",
        "    mask = layers.Masking(mask_value=0)\n",
        "    def _combine_and_sum(args):\n",
        "        [embedding, idf] = args\n",
        "        return K.sum(embedding * K.abs(idf), axis=1)\n",
        "\n",
        "    sum_layer = layers.Lambda(_combine_and_sum, name='combine_and_sum')\n",
        "\n",
        "    sum_a = sum_layer([mask(embedding_a(title)), idf_a(title)])\n",
        "    sum_b = sum_layer([mask(embedding_b(body)), idf_b(body)])\n",
        "\n",
        "    sim = layers.dot([sum_a, sum_b], axes=1, normalize=True)\n",
        "    sim_model = models.Model(\n",
        "        inputs=[title, body],\n",
        "        outputs=[sim],\n",
        "    )\n",
        "    sim_model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "    sim_model.summary()\n",
        "\n",
        "    embedding_model = models.Model(\n",
        "        inputs=[title],\n",
        "        outputs=[sum_a]\n",
        "    )\n",
        "    return sim_model, embedding_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48PJsmsky0iJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try using our model with pretrained weights from word2vec\n",
        "\n",
        "sum_model_precomputed, sum_embedding_precomputed = sum_model(\n",
        "    embedding_size=EMBEDDING_SIZE, vocab_size=VOCAB_SIZE,\n",
        "    embedding_weights=w2v_weights, idf_weights=idf_weights\n",
        ")\n",
        "\n",
        "x, y = next(data_generator(batch_size=4096))\n",
        "sum_model_precomputed.evaluate(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYbnkyj3zBUg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "a7b9437e-62d4-481d-f35d-a0101a750d61"
      },
      "source": [
        "# Try using our model with pretrained weights from word2vec\n",
        "\n",
        "sum_model_precomputed, sum_embedding_precomputed = sum_model(\n",
        "    embedding_size=EMBEDDING_SIZE, vocab_size=VOCAB_SIZE,\n",
        "    embedding_weights=w2v_weights, idf_weights=idf_weights\n",
        ")\n",
        "\n",
        "x, y = next(data_generator(batch_size=4096))\n",
        "sum_model_precomputed.evaluate(x, y)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c3a94fc52094>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m sum_model_precomputed, sum_embedding_precomputed = sum_model(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBEDDING_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0membedding_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw2v_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midf_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midf_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sum_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGT5BukkzIwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SAMPLE_QUESTIONS = [\n",
        "    'Roundtrip ticket versus one way',\n",
        "    'Shinkansen from Kyoto to Hiroshima',\n",
        "    'Bus tour of Germany',\n",
        "]\n",
        "\n",
        "def evaluate_sample(lookup):\n",
        "    pd.set_option('display.max_colwidth', 100)\n",
        "    results = []\n",
        "    for q in SAMPLE_QUESTIONS:\n",
        "        print(q)\n",
        "        q_res = lookup.nearest(q, n=4)\n",
        "        q_res['result'] = q_res['question']\n",
        "        q_res['question'] = q\n",
        "        results.append(q_res)\n",
        "\n",
        "    return pd.concat(results)\n",
        "\n",
        "lookup = EmbeddingWrapper(model=sum_embedding_precomputed)\n",
        "evaluate_sample(lookup)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5_TUEElz8js",
        "colab_type": "text"
      },
      "source": [
        "**Training our own network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KMY0J-Mz5Qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum_model_trained, sum_embedding_trained = sum_model(\n",
        "    embedding_size=EMBEDDING_SIZE, vocab_size=VOCAB_SIZE, \n",
        "    embedding_weights=None,\n",
        "    idf_weights=None\n",
        ")\n",
        "sum_model_trained.fit_generator(\n",
        "    data_generator(batch_size=128),\n",
        "    epochs=10,\n",
        "    steps_per_epoch=1000\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoVr2Zdw0NYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lookup = EmbeddingWrapper(model=sum_embedding_trained)\n",
        "evaluate_sample(lookup)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrShdNXh0cRu",
        "colab_type": "text"
      },
      "source": [
        "**CNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hj-NiyI0bKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cnn_model(embedding_size, vocab_size):\n",
        "    title = layers.Input(shape=(None,), dtype='int32', name='title')\n",
        "    body = layers.Input(shape=(None,), dtype='int32', name='body')\n",
        "\n",
        "    embedding = layers.Embedding(\n",
        "        mask_zero=False,\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_size,\n",
        "    )\n",
        "\n",
        "\n",
        "    def _combine_sum(v):\n",
        "        return K.sum(v, axis=1)\n",
        "\n",
        "    cnn_1 = layers.Convolution1D(256, 3)\n",
        "    cnn_2 = layers.Convolution1D(256, 3)\n",
        "    cnn_3 = layers.Convolution1D(256, 3)\n",
        "    \n",
        "    global_pool = layers.GlobalMaxPooling1D()\n",
        "    local_pool = layers.MaxPooling1D(strides=2, pool_size=3)\n",
        "\n",
        "    def forward(input):\n",
        "        embed = embedding(input)\n",
        "        return global_pool(\n",
        "            cnn_2(local_pool(cnn_1(embed))))\n",
        "\n",
        "    sum_a = forward(title)\n",
        "    sum_b = forward(body)\n",
        "\n",
        "    sim = layers.dot([sum_a, sum_b], axes=1, normalize=False)\n",
        "    sim_model = models.Model(\n",
        "        inputs=[title, body],\n",
        "        outputs=[sim],\n",
        "    )\n",
        "    sim_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "    embedding_model = models.Model(\n",
        "        inputs=[title],\n",
        "        outputs=[sum_a]\n",
        "    )\n",
        "    return sim_model, embedding_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UH5zSUh0zHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn, cnn_embedding = cnn_model(embedding_size=25, vocab_size=VOCAB_SIZE)\n",
        "cnn.summary()\n",
        "cnn.fit_generator(\n",
        "    data_generator(batch_size=128),\n",
        "    epochs=10,\n",
        "    steps_per_epoch=1000,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZSZUhqS08V3",
        "colab_type": "text"
      },
      "source": [
        "LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8oWt9pw1CdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_model(embedding_size, vocab_size):\n",
        "    title = layers.Input(shape=(None,), dtype='int32', name='title')\n",
        "    body = layers.Input(shape=(None,), dtype='int32', name='body')\n",
        "\n",
        "    embedding = layers.Embedding(\n",
        "        mask_zero=True,\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_size,\n",
        "#         weights=[w2v_weights],\n",
        "#         trainable=False\n",
        "    )\n",
        "\n",
        "    lstm_1 = layers.LSTM(units=512, return_sequences=True)\n",
        "    lstm_2 = layers.LSTM(units=512, return_sequences=False)\n",
        "    \n",
        "    sum_a = lstm_2(lstm_1(embedding(title)))\n",
        "    sum_b = lstm_2(lstm_1(embedding(body)))\n",
        "\n",
        "    sim = layers.dot([sum_a, sum_b], axes=1, normalize=True)\n",
        "#     sim = layers.Activation(activation='sigmoid')(sim)\n",
        "    sim_model = models.Model(\n",
        "        inputs=[title, body],\n",
        "        outputs=[sim],\n",
        "    )\n",
        "    sim_model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "    embedding_model = models.Model(\n",
        "        inputs=[title],\n",
        "        outputs=[sum_a]\n",
        "    )\n",
        "    return sim_model, embedding_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_8hDpJJ1O0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm, lstm_embedding = lstm_model(embedding_size=EMBEDDING_SIZE, vocab_size=VOCAB_SIZE)\n",
        "lstm.summary()\n",
        "lstm.fit_generator(\n",
        "    data_generator(batch_size=128),\n",
        "    epochs=10,\n",
        "    steps_per_epoch=100,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyMbavM11RW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lookup = EmbeddingWrapper(model=lstm_embedding)\n",
        "evaluate_sample(lookup)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}